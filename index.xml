<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Omid Taheri</title>
    <link>https://otaheri.github.io/</link>
      <atom:link href="https://otaheri.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Omid Taheri</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 07 Apr 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://otaheri.github.io/media/icon_hu9b7be29231a05e8610b49408e09cf277_125589_512x512_fill_lanczos_center_3.png</url>
      <title>Omid Taheri</title>
      <link>https://otaheri.github.io/</link>
    </image>
    
    <item>
      <title>InteractVLM: 3D Interaction Reasoning from 2D Foundational Models</title>
      <link>https://otaheri.github.io/publication/2025_intervlm/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2025_intervlm/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;We introduce &lt;strong&gt;InteractVLM&lt;/strong&gt;, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. We propose a &lt;strong&gt;Render-Localize-Lift&lt;/strong&gt; module that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Render:&lt;/strong&gt; Embeds 3D body and object surfaces into 2D space via multi-view rendering.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Localize:&lt;/strong&gt; Trains a multi-view localization model (MV-Loc) to infer precise 2D contact points.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lift:&lt;/strong&gt; Projects the localized 2D contacts back to the 3D mesh.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Additionally, we define a new task—&lt;strong&gt;Semantic Human Contact&lt;/strong&gt;—which conditions contact estimation on object semantics, going beyond binary labels to infer object-specific interaction regions. InteractVLM significantly outperforms prior art on contact estimation benchmarks and facilitates joint 3D reconstruction from a single image.&lt;/p&gt;
&lt;h1 id=&#34;teaser&#34;&gt;Teaser&lt;/h1&gt;
&lt;img src=&#34;videos/teaser.jpg&#34; class=&#34;center&#34; /&gt;
&lt;div style=&#34;text-align:center; margin-top:10px;&#34;&gt;
  &lt;span style=&#34;color:#007acc; font-weight:bold;&#34;&gt;InteractVLM&lt;/span&gt; estimates 3D contact points on both human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D.
  We introduce **Semantic Human Contact**, inferring object-specific contacts on the body, and leverage large Vision-Language Models for better generalization to diverse real-world interactions.
&lt;/div&gt;
&lt;h1 id=&#34;joint-humanobject-reconstruction&#34;&gt;Joint Human–Object Reconstruction&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Accurate joint 3D reconstruction of human and object from a single image.&lt;/strong&gt;&lt;/p&gt;
&lt;video autoplay muted loop playsinline height=&#34;300&#34;&gt;
  &lt;source src=&#34;videos/joint_reconstruction.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;h1 id=&#34;method-overview&#34;&gt;Method Overview&lt;/h1&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Method Overview&#34; srcset=&#34;
               /publication/2025_intervlm/videos/method_hu922168d7cfb8605d8c39fbecbc354498_292851_70ff8ef1363f915d952cf7c4971c2eb7.webp 400w,
               /publication/2025_intervlm/videos/method_hu922168d7cfb8605d8c39fbecbc354498_292851_1d3f4671cd6450143828026c5f9bf9ca.webp 760w,
               /publication/2025_intervlm/videos/method_hu922168d7cfb8605d8c39fbecbc354498_292851_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://otaheri.github.io/publication/2025_intervlm/videos/method_hu922168d7cfb8605d8c39fbecbc354498_292851_70ff8ef1363f915d952cf7c4971c2eb7.webp&#34;
               width=&#34;760&#34;
               height=&#34;308&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;comparison-interactvlm-vs-phosa&#34;&gt;Comparison: InteractVLM vs. PHOSA&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Joint Reconstruction Comparison&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&#34;display:flex; gap:10px; justify-content:center; flex-wrap: wrap;&#34;&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/bench.png&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;Input Image&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/bench_phosa.gif&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;PHOSA&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/bench_ivlm.gif&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;InteractVLM&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;semantic-human-contact-estimation&#34;&gt;Semantic Human Contact Estimation&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Contact Estimation Comparison&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&#34;display:flex; gap:10px; justify-content:center; flex-wrap: wrap;&#34;&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/hcontact_bench.jpg&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;Input Image&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/hcontact_bench_deco.gif&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;DECO&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/hcontact_bench_ivlm.gif&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;InteractVLM&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;object-affordance-prediction&#34;&gt;Object Affordance Prediction&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Affordance Comparison&lt;/strong&gt;&lt;/p&gt;
&lt;div style=&#34;display:flex; gap:10px; justify-content:center; flex-wrap: wrap;&#34;&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/ocontact_img1.png&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;Input Image&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/ocontact_img1_piad.png&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;PIAD&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/ocontact_img1_ivlm.png&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;InteractVLM&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;summary-video&#34;&gt;Summary Video&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
  &lt;iframe width=&#34;560&#34; height=&#34;315&#34;
    src=&#34;https://www.youtube.com/embed/brxygxM1nRk&#34;
    frameborder=&#34;0&#34;
    allow=&#34;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34;
    allowfullscreen&gt;
  &lt;/iframe&gt;
&lt;/div&gt;
&lt;h1 id=&#34;acknowledgments--disclosure&#34;&gt;Acknowledgments &amp;amp; Disclosure&lt;/h1&gt;
&lt;p&gt;We thank Alpár Cseke for assistance with joint reconstruction evaluation; Tsvetelina Alexiadis and Taylor Obersat for MTurk studies; Yao Feng, Peter Kulits, Markos Diomataris for feedback; and Benjamin Pellkofer for IT support. SKD is funded by IMPRS-IS; UvA work by ERC Starting Grant STRIPES (101165317). DT received Google research funding. MJB’s involvement was solely supported by the Max Planck Society.&lt;/p&gt;
&lt;h1 id=&#34;contact&#34;&gt;Contact&lt;/h1&gt;
&lt;p&gt;For technical questions: &lt;a href=&#34;mailto:sai.dwivedi@tue.mpg.de&#34;&gt;sai.dwivedi@tue.mpg.de&lt;/a&gt;&lt;br&gt;
For licensing: &lt;a href=&#34;mailto:ps-licensing@tue.mpg.de&#34;&gt;ps-licensing@tue.mpg.de&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;bibtex&#34;&gt;BibTeX&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nc&#34;&gt;@inproceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;dwivedi_interactvlm_2025&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;title&lt;/span&gt;     &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{{InteractVLM}: {3D} Interaction Reasoning from {2D} Foundational Models}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;author&lt;/span&gt;    &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Dwivedi, Sai Kumar and Antić, Dimitrije and Tripathi, Shashank and Taheri, Omid and Schmid, Cordelia and Black, Michael J. and Tzionas, Dimitrios}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;booktitle&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;month&lt;/span&gt;     &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{June}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;year&lt;/span&gt;      &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{2025}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Humanity&#39;s Last Exam: A Multi-Modal Benchmark at the Frontier of Human Knowledge</title>
      <link>https://otaheri.github.io/publication/2025_lastexam/</link>
      <pubDate>Thu, 03 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2025_lastexam/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Benchmarks are essential for tracking rapid LLM progress—but today’s models exceed 90% on tasks like MMLU, saturating existing exams.&lt;br&gt;
We introduce &lt;strong&gt;Humanity’s Last Exam (HLE)&lt;/strong&gt;, a &lt;strong&gt;multi-modal&lt;/strong&gt;, &lt;strong&gt;closed-ended&lt;/strong&gt; benchmark spanning &lt;strong&gt;2,500&lt;/strong&gt; questions across &lt;strong&gt;100+&lt;/strong&gt; subjects at the frontier of human knowledge.&lt;br&gt;
All questions are publicly released; a held-out private test set prevents overfitting.&lt;/p&gt;
&lt;h1 id=&#34;teaser&#34;&gt;Teaser&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
  &lt;img src=&#34;videos/difficulty_comparison.png&#34; class=&#34;center&#34; alt=&#34;Difficulty comparison across benchmarks&#34; style=&#34;max-width:100%;&#34;/&gt;
  &lt;p style=&#34;margin-top:10px; font-size:1.1em;&#34;&gt;
    Even the strongest frontier LLMs peak at &lt;strong&gt;~21.6%&lt;/strong&gt; accuracy on HLE, leaving ample headroom for progress.
  &lt;/p&gt;
&lt;/div&gt;
&lt;h1 id=&#34;benchmark-design&#34;&gt;Benchmark Design&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Question Collection:&lt;/strong&gt; Curated 2,500 closed-ended items covering STEM, humanities, arts, and professional domains.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Modal Inputs:&lt;/strong&gt; Includes diagrams, charts, and short passages—tests both visual and textual reasoning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluation Protocol:&lt;/strong&gt; Public training/dev split; private test set for leaderboard submissions to guard against over-tuning.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;quantitative-results&#34;&gt;Quantitative Results&lt;/h1&gt;
&lt;div style=&#34;display:flex; gap:2rem; flex-wrap:wrap; justify-content:center;&#34;&gt;
  &lt;div style=&#34;flex:1; min-width:280px;&#34;&gt;
    &lt;p&gt;&lt;strong&gt;Accuracy.&lt;/strong&gt; State-of-the-art LLMs score between 2.7% and 21.6% on HLE.&lt;/p&gt;
    &lt;p&gt;&lt;strong&gt;Calibration Error.&lt;/strong&gt; We also prompt each model for a confidence score (0–100%) to quantify over-confidence in wrong answers.&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 600px; overflow-x:auto;&#34;&gt;
    &lt;div class=&#34;mb-2 text-xs text-gray-600 text-right&#34;&gt;
      Judge Model: o3-mini | Dataset Updated: April 3rd, 2025
    &lt;/div&gt;
    &lt;table class=&#34;w-full caption-bottom text-sm border&#34;&gt;
      &lt;thead&gt;
        &lt;tr class=&#34;border-b&#34;&gt;
          &lt;th class=&#34;h-12 px-4 text-left&#34;&gt;Model&lt;/th&gt;
          &lt;th class=&#34;h-12 px-4 text-right&#34;&gt;Accuracy&amp;nbsp;(%)&amp;nbsp;↑&lt;/th&gt;
          &lt;th class=&#34;h-12 px-4 text-center&#34;&gt;Calibration&lt;br&gt;Error&amp;nbsp;(%)&amp;nbsp;↓&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;&lt;td&gt;Gemini 2.5 Pro&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;21.6&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;72.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;o3&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;20.3&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;34.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;o4-mini&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;18.1&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;57.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;DeepSeek-R1-0528*&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;14.0&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;78.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;o3-mini*&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;13.4&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;80.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;Gemini 2.5 Flash&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;12.1&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;80.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;Qwen3-235B*&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;11.8&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;74.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;Claude 4 Opus&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;10.7&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;73.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;DeepSeek-R1*&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;8.5&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;73.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;Claude 3.7 Sonnet&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;8.0&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;80.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;o1&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;8.0&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;83.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;Claude 4 Sonnet&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;7.7&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;80.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;Llama 4 Maverick&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;5.7&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;83.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;GPT-4.5 Preview&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;5.4&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;85.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;GPT-4.1&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;5.4&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;89.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;Claude 3.5 Sonnet&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;4.1&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;84.0&lt;/td&gt;&lt;/tr&gt;
        &lt;tr&gt;&lt;td&gt;GPT-4o&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;2.7&lt;/td&gt;&lt;td class=&#34;text-center&#34;&gt;89.0&lt;/td&gt;&lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
    &lt;p class=&#34;mt-2 text-xs text-gray-600&#34;&gt;*Not multi-modal; evaluated on text-only subset.&lt;/p&gt;
    &lt;p class=&#34;mt-1 text-xs text-gray-600&#34;&gt;
      Available on the &lt;a href=&#34;https://scale.com/leaderboard/humanitys_last_exam&#34; target=&#34;_blank&#34;&gt;SEAL LLM Leaderboards&lt;/a&gt;.
    &lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;leaderboards--comparisons&#34;&gt;Leaderboards &amp;amp; Comparisons&lt;/h1&gt;
&lt;p&gt;In addition to our private test, HLE is live on SEAL’s public leaderboard.&lt;br&gt;
Compared to MMLU (⟨90%⟩), ARC, and other academic exams, HLE remains unsolved—providing a durable benchmark.&lt;/p&gt;
&lt;h1 id=&#34;discussion&#34;&gt;Discussion&lt;/h1&gt;
&lt;p&gt;Current frontier models score under 22% and exhibit high calibration errors, underscoring both capability and confidence gaps.&lt;br&gt;
Historically, benchmarks saturate rapidly—models could surpass 50% on HLE by late 2025, but expert-level, multi-modal closed-ended reasoning will remain a challenge beyond that point.&lt;/p&gt;
&lt;h1 id=&#34;bibtex&#34;&gt;BibTeX&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nc&#34;&gt;@misc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;phan2025humanitysexam&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;na&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;{Humanity&amp;#39;s Last Exam}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;na&#34;&gt;author&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;{Long Phan and Alice Gatti and Ziwen Han and Nathaniel Li and Josephina Hu and Hugh Zhang and Chen Bo Calvin Zhang and Mohamed Shaaban and John Ling and Sean Shi and Michael Choi and Anish Agrawal and Arnav Chopra and Adam Khoja and Ryan Kim and Richard Ren and Jason Hausenloy and Oliver Zhang and Mantas Mazeika and Dmitry Dodonov and Tung Nguyen and Jaeho Lee and Daron Anderson and Mikhail Doroshenko and Alun Cennyth Stokes and Mobeen Mahmood and Oleksandr Pokutnyi and Oleg Iskra and Jessica P. Wang and John-Clark Levin and Mstyslav Kazakov and Fiona Feng and Steven Y. Feng and Haoran Zhao and Michael Yu and Varun Gangal and Chelsea Zou and Zihan Wang and Serguei Popov and Robert Gerbicz and Geoff Galgon and Johannes Schmitt and Will Yeadon and Yongki Lee and Scott Sauers and Alvaro Sanchez and Fabian Giska and Marc Roth and Søren Riis and Saiteja Utpala and Noah Burns and Gashaw M. Goshu and Mohinder Maheshbhai Naiya and Chidozie Agu and Zachary Giboney and Antrell Cheatom and Francesco Fournier-Facio and Sarah-Jane Crowson and Lennart Finke and Zerui Cheng and Jennifer Zampese and Ryan G. Hoerr and Mark Nandor and Hyunwoo Park and Tim Gehrunger and Jiaqi Cai and Ben McCarty and Alexis C Garretson and Edwin Taylor and Damien Sileo and Qiuyu Ren and Usman Qazi and Lianghui Li and Jungbae Nam and John B. Wydallis and Pavel Arkhipov and Jack Wei Lun Shi and Aras Bacho and Chris G. Willcocks and Hangrui Cao and Sumeet Motwani and Emily de Oliveira Santos and Johannes Veith and Edward Vendrow and Doru Cojoc and Kengo Zenitani and Joshua Robinson and Longke Tang and Yuqi Li and Joshua Vendrow and Natanael Wildner Fraga and Vladyslav Kuchkin and Andrey Pupasov Maksimov and Pierre Marion and Denis Efremov and Jayson Lynch and Kaiqu Liang and Aleksandar Mikov and Andrew Gritsevskiy and Julien Guillod and Gözdenur Demir and Dakotah Martinez and Ben Pageler and Kevin Zhou and Saeed Soori and Ori Press and Henry Tang and Paolo Rissone and Sean R. Green and Lina Brüssel and Moon Twayana and Aymeric Dieuleveut and Joseph Marvin Imperial and Ameya Prabhu and Jinzhou Yang and Nick Crispino and Arun Rao and Dimitri Zvonkine and Gabriel Loiseau and Mikhail Kalinin and Marco Lukas and Ciprian Manolescu and Nate Stambaugh and Subrata Mishra and Tad Hogg and Carlo Bosio and Brian P Coppola and Julian Salazar and Jaehyeok Jin and Rafael Sayous and Stefan Ivanov and Philippe Schwaller and Shaipranesh Senthilkuma and Andres M Bran and Andres Algaba and Kelsey Van den Houte and Lynn Van Der Sypt and Brecht Verbeken and David Noever and Alexei Kopylov and Benjamin Myklebust and Bikun Li and Lisa Schut and Evgenii Zheltonozhskii and Qiaochu Yuan and Derek Lim and Richard Stanley and Tong Yang and John Maar and Julian Wykowski and Martí Oller and Anmol Sahu and Cesare Giulio Ardito and Yuzheng Hu and Ariel Ghislain Kemogne Kamdoum and Alvin Jin and Tobias Garcia Vilchis and Yuexuan Zu and Martin Lackner and James Koppel and Gongbo Sun and Daniil S. Antonenko and Steffi Chern and Bingchen Zhao and Pierrot Arsene and Joseph M Cavanagh and Daofeng Li and Jiawei Shen and Donato Crisostomi and Wenjin Zhang and Ali Dehghan and Sergey Ivanov and David Perrella and Nurdin Kaparov and Allen Zang and Ilia Sucholutsky and Arina Kharlamova and Daniil Orel and Vladislav Poritski and Shalev Ben-David and Zachary Berger and Parker Whitfill and Michael Foster and Daniel Munro and Linh Ho and Shankar Sivarajan and Dan Bar Hava and Aleksey Kuchkin and David Holmes and Alexandra Rodriguez-Romero and Frank Sommerhage and Anji Zhang and Richard Moat and Keith Schneider and Zakayo Kazibwe and Don Clarke and Dae Hyun Kim and Felipe Meneguitti Dias and Sara Fish and Veit Elser and Tobias Kreiman and Victor Efren Guadarrama Vilchis and Immo Klose and Ujjwala Anantheswaran and Adam Zweiger and Kaivalya Rawal and Jeffery Li and Jeremy Nguyen and Nicolas Daans and Haline Heidinger and Maksim Radionov and Václav Rozhoň and Vincent Ginis and Christian Stump and Niv Cohen and Rafał Poświata and Josef Tkadlec and Alan Goldfarb and Chenguang Wang and Piotr Padlewski and Stanislaw Barzowski and Kyle Montgomery and Ryan Stendall and Jamie Tucker-Foltz and Jack Stade and T. Ryan Rogers and Tom Goertzen and Declan Grabb and Abhishek Shukla and Alan Givré and John Arnold Ambay and Archan Sen and Muhammad Fayez Aziz and Mark H Inlow and Hao He and Ling Zhang and Younesse Kaddar and Ivar Ängquist and Yanxu Chen and Harrison K Wang and Kalyan Ramakrishnan and Elliott Thornley and Antonio Terpin and Hailey Schoelkopf and Eric Zheng and Avishy Carmi and Ethan D. L. Brown and Kelin Zhu and Max Bartolo and Richard Wheeler and Martin Stehberger and Peter Bradshaw and JP Heimonen and Kaustubh Sridhar and Ido Akov and Jennifer Sandlin and Yury Makarychev and Joanna Tam and Hieu Hoang and David M. Cunningham and Vladimir Goryachev and Demosthenes Patramanis and Michael Krause and Andrew Redenti and David Aldous and Jesyin Lai and Shannon Coleman and Jiangnan Xu and Sangwon Lee and Ilias Magoulas and Sandy Zhao and Ning Tang and Michael K. Cohen and Orr Paradise and Jan Hendrik Kirchner and Maksym Ovchynnikov and Jason O. Matos and Adithya Shenoy and Michael Wang and Yuzhou Nie and Anna Sztyber-Betley and Paolo Faraboschi and Robin Riblet and Jonathan Crozier and Shiv Halasyamani and Shreyas Verma and Prashant Joshi and Eli Meril and Ziqiao Ma and Jérémy Andréoletti and Raghav Singhal and Jacob Platnick and Volodymyr Nevirkovets and Luke Basler and Alexander Ivanov and Seri Khoury and Nils Gustafsson and Marco Piccardo and Hamid Mostaghimi and Qijia Chen and Virendra Singh and Tran Quoc Khánh and Paul Rosu and Hannah Szlyk and Zachary Brown and Himanshu Narayan and Aline Menezes and Jonathan Roberts and William Alley and Kunyang Sun and Arkil Patel and Max Lamparth and Anka Reuel and Linwei Xin and Hanmeng Xu and Jacob Loader and Freddie Martin and Zixuan Wang and Andrea Achilleos and Thomas Preu and Tomek Korbak and Ida Bosio and Fereshteh Kazemi and Ziye Chen and Biró Bálint and Eve J. Y. Lo and Jiaqi Wang and Maria Inês S. Nunes and Jeremiah Milbauer and M Saiful Bari and Zihao Wang and Behzad Ansarinejad and Yewen Sun and Stephane Durand and Hossam Elgnainy and Guillaume Douville and Daniel Tordera and George Balabanian and Hew Wolff and Lynna Kvistad and Hsiaoyun Milliron and Ahmad Sakor and Murat Eron and Andrew Favre D. O. and Shailesh Shah and Xiaoxiang Zhou and Firuz Kamalov and Sherwin Abdoli and Tim Santens and Shaul Barkan and Allison Tee and Robin Zhang and Alessandro Tomasiello and G. Bruno De Luca and Shi-Zhuo Looi and Vinh-Kha Le and Noam Kolt and Jiayi Pan and Emma Rodman and Jacob Drori and Carl J Fossum and Niklas Muennighoff and Milind Jagota and Ronak Pradeep and Honglu Fan and Jonathan Eicher and Michael Chen and Kushal Thaman and William Merrill and Moritz Firsching and Carter Harris and Stefan Ciobâcă and Jason Gross and Rohan Pandey and Ilya Gusev and Adam Jones and Shashank Agnihotri and Pavel Zhelnov and Mohammadreza Mofayezi and Alexander Piperski and David K. Zhang and Kostiantyn Dobarskyi and Roman Leventov and Ignat Soroko and Joshua Duersch and Vage Taamazyan and Andrew Ho and Wenjie Ma and William Held and Ruicheng Xian and Armel Randy Zebaze and Mohanad Mohamed and Julian Noah Leser and Michelle X Yuan and Laila Yacar and Johannes Lengler and Katarzyna Olszewska and Claudio Di Fratta and Edson Oliveira and Joseph W. Jackson and Andy Zou and Muthu Chidambaram and Timothy Manik and Hector Haffenden and Dashiell Stander and Ali Dasouqi and Alexander Shen and Bita Golshani and David Stap and Egor Kretov and Mikalai Uzhou and Alina Borisovna Zhidkovskaya and Nick Winter and Miguel Orbegozo Rodriguez and Robert Lauff and Dustin Wehr and Colin Tang and Zaki Hossain and Shaun Phillips and Fortuna Samuele and Fredrik Ekström and Angela Hammon and Oam Patel and Faraz Farhidi and George Medley and Forough Mohammadzadeh and Madellene Peñaflor and Haile Kassahun and Alena Friedrich and Rayner Hernandez Perez and Daniel Pyda and Taom Sakal and Omkar Dhamane and Ali Khajegili Mirabadi and Eric Hallman and Kenchi Okutsu and Mike Battaglia and Mohammad Maghsoudimehrabani and Alon Amit and Dave Hulbert and Roberto Pereira and Simon Weber and Handoko and Anton Peristyy and Stephen Malina and Mustafa Mehkary and Rami Aly and Frank Reidegeld and Anna-Katharina Dick and Cary Friday and Mukhwinder Singh and Hassan Shapourian and Wanyoung Kim and Mariana Costa and Hubeyb Gurdogan and Harsh Kumar and Chiara Ceconello and Chao Zhuang and Haon Park and Micah Carroll and Andrew R. Tawfeek and Stefan Steinerberger and Daattavya Aggarwal and Michael Kirchhof and Linjie Dai and Evan Kim and Johan Ferret and Jainam Shah and Yuzhou Wang and Minghao Yan and Krzysztof Burdzy and Lixin Zhang and Antonio Franca and Diana T. Pham and Kang Yong Loh and Joshua Robinson and Abram Jackson and Paolo Giordano and Philipp Petersen and Adrian Cosma and Jesus Colino and Colin White and Jacob Votava and Vladimir Vinnikov and Ethan Delaney and Petr Spelda and Vit Stritecky and Syed M. Shahid and Jean-Christophe Mourrat and Lavr Vetoshkin and Koen Sponselee and Renas Bacho and Zheng-Xin Yong and Florencia de la Rosa and Nathan Cho and Xiuyu Li and Guillaume Malod and Orion Weller and Guglielmo Albani and Leon Lang and Julien Laurendeau and Dmitry Kazakov and Fatimah Adesanya and Julien Portier and Lawrence Hollom and Victor Souza and Yuchen Anna Zhou and Julien Degorre and Yiğit Yalın and Gbenga Daniel Obikoya and Rai and Filippo Bigi and M. C. Boscá and Oleg Shumar and Kaniuar Bacho and Gabriel Recchia and Mara Popescu and Nikita Shulga and Ngefor Mildred Tanwie and Thomas C. H. Lux and Ben Rank and Colin Ni and Matthew Brooks and Alesia Yakimchyk and Huanxu and Liu and Stefano Cavalleri and Olle Häggström and Emil Verkama and Joshua Newbould and Hans Gundlach and Leonor Brito-Santana and Brian Amaro and Vivek Vajipey and Rynaa Grover and Ting Wang and Yosi Kratish and Wen-Ding Li and Sivakanth Gopi and Andrea Caciolai and Christian Schroeder de Witt and Pablo Hernández-Cámara and Emanuele Rodolà and Jules Robins and Dominic Williamson and Vincent Cheng and Brad Raynor and Hao Qi and Ben Segev and Jingxuan Fan and Sarah Martinson and Erik Y. Wang and Kaylie Hausknecht and Michael P. Brenner and Mao Mao and Christoph Demian and Peyman Kassani and Xinyu Zhang and David Avagian and Eshawn Jessica Scipio and Alon Ragoler and Justin Tan and Blake Sims and Rebeka Plecnik and Aaron Kirtland and Omer Faruk Bodur and D. P. Shinde and Yan Carlos Leyva Labrador and Zahra Adoul and Mohamed Zekry and Ali Karakoc and Tania C. B. Santos and Samir Shamseldeen and Loukmane Karim and Anna Liakhovitskaia and Nate Resman and Nicholas Farina and Juan Carlos Gonzalez and Gabe Maayan and Earth Anderson and Rodrigo De Oliveira Pena and Elizabeth Kelley and Hodjat Mariji and Rasoul Pouriamanesh and Wentao Wu and Ross Finocchio and Ismail Alarab and Joshua Cole and Danyelle Ferreira and Bryan Johnson and Mohammad Safdari and Liangti Dai and Siriphan Arthornthurasuk and Isaac C. McAlister and Alejandro José Moyano and Alexey Pronin and Jing Fan and Angel Ramirez-Trinidad and Yana Malysheva and Daphiny Pottmaier and Omid Taheri and Stanley Stepanic and Samuel Perry and Luke Askew and Raúl Adrián Huerta Rodríguez and Ali M. R. Minissi and Ricardo Lorena and Krishnamurthy Iyer and Arshad Anil Fasiludeen and Ronald Clark and Josh Ducey and Matheus Piza and Maja Somrak and Eric Vergo and Juehang Qin and Benjámin Borbás and Eric Chu and Jack Lindsey and Antoine Jallon and I. M. J. McInnis and Evan Chen and Avi Semler and Luk Gloor and Tej Shah and Marc Carauleanu and Pascal Lauer and Tran Đuc Huy and Hossein Shahrtash and Emilien Duc and Lukas Lewark and Assaf Brown and Samuel Albanie and Brian Weber and Warren S. Vaz and Pierre Clavier and Yiyang Fan and Gabriel Poesia Reis e Silva and Long and Lian and Marcus Abramovitch and Xi Jiang and Sandra Mendoza and Murat Islam and Juan Gonzalez and Vasilios Mavroudis and Justin Xu and Pawan Kumar and Laxman Prasad Goswami and Daniel Bugas and Nasser Heydari and Ferenc Jeanplong and Thorben Jansen and Antonella Pinto and Archimedes Apronti and Abdallah Galal and Ng Ze-An and Ankit Singh and Tong Jiang and Joan of Arc Xavier and Kanu Priya Agarwal and Mohammed Berkani and Gang Zhang and Zhehang Du and Benedito Alves de Oliveira Junior and Dmitry Malishev and Nicolas Remy and Taylor D. Hartman and Tim Tarver and Stephen Mensah and Gautier Abou Loume and Wiktor Morak and Farzad Habibi and Sarah Hoback and Will Cai and Javier Gimenez and Roselynn Grace Montecillo and Jakub Łucki and Russell Campbell and Asankhaya Sharma and Khalida Meer and Shreen Gul and Daniel Espinosa Gonzalez and Xavier Alapont and Alex Hoover and Gunjan Chhablani and Freddie Vargus and Arunim Agarwal and Yibo Jiang and Deepakkumar Patil and David Outevsky and Kevin Joseph Scaria and Rajat Maheshwari and Abdelkader Dendane and Priti Shukla and Ashley Cartwright and Sergei Bogdanov and Niels Mündler and Sören Möller and Luca Arnaboldi and Kunvar Thaman and Muhammad Rehan Siddiqi and Prajvi Saxena and Himanshu Gupta and Tony Fruhauff and Glen Sherman and Mátyás Vincze and Siranut Usawasutsakorn and Dylan Ler and Anil Radhakrishnan and Innocent Enyekwe and Sk Md Salauddin and Jiang Muzhen and Aleksandr Maksapetyan and Vivien Rossbach and Chris Harjadi and Mohsen Bahaloohoreh and Claire Sparrow and Jasdeep Sidhu and Sam Ali and Song Bian and John Lai and Eric Singer and Justine Leon Uro and Greg Bateman and Mohamed Sayed and Ahmed Menshawy and Darling Duclosel and Dario Bezzi and Yashaswini Jain and Ashley Aaron and Murat Tiryakioglu and Sheeshram Siddh and Keith Krenek and Imad Ali Shah and Jun Jin and Scott Creighton and Denis Peskoff and Zienab EL-Wasif and Ragavendran P V and Michael Richmond and Joseph McGowan and Tejal Patwardhan and Hao-Yu Sun and Ting Sun and Nikola Zubić and Samuele Sala and Stephen Ebert and Jean Kaddour and Manuel Schottdorf and Dianzhuo Wang and Gerol Petruzella and Alex Meiburg and Tilen Medved and Ali ElSheikh and S Ashwin Hebbar and Lorenzo Vaquero and Xianjun Yang and Jason Poulos and Vilém Zouhar and Sergey Bogdanik and Mingfang Zhang and Jorge Sanz-Ros and David Anugraha and Yinwei Dai and Anh N. Nhu and Xue Wang and Ali Anil Demircali and Zhibai Jia and Yuyin Zhou and Juncheng Wu and Mike He and Nitin Chandok and Aarush Sinha and Gaoxiang Luo and Long Le and Mickaël Noyé and Michał Perełkiewicz and Ioannis Pantidis and Tianbo Qi and Soham Sachin Purohit and Letitia Parcalabescu and Thai-Hoa Nguyen and Genta Indra Winata and Edoardo M. Ponti and Hanchen Li and Kaustubh Dhole and Jongee Park and Dario Abbondanza and Yuanli Wang and Anupam Nayak and Diogo M. Caetano and Antonio A. W. L. Wong and Maria del Rio-Chanona and Dániel Kondor and Pieter Francois and Ed Chalstrey and Jakob Zsambok and Dan Hoyer and Jenny Reddish and Jakob Hauser and Francisco-Javier Rodrigo-Ginés and Suchandra Datta and Maxwell Shepherd and Thom Kamphuis and Qizheng Zhang and Hyunjun Kim and Ruiji Sun and Jianzhu Yao and Franck Dernoncourt and Satyapriya Krishna and Sina Rismanchian and Bonan Pu and Francesco Pinto and Yingheng Wang and Kumar Shridhar and Kalon J. Overholt and Glib Briia and Hieu Nguyen and David and Soler Bartomeu and Tony CY Pang and Adam Wecker and Yifan Xiong and Fanfei Li and Lukas S. Huber and Joshua Jaeger and Romano De Maddalena and Xing Han Lù and Yuhui Zhang and Claas Beger and Patrick Tser Jern Kon and Sean Li and Vivek Sanker and Ming Yin and Yihao Liang and Xinlu Zhang and Ankit Agrawal and Li S. Yifei and Zechen Zhang and Mu Cai and Yasin Sonmez and Costin Cozianu and Changhao Li and Alex Slen and Shoubin Yu and Hyun Kyu Park and Gabriele Sarti and Marcin Briański and Alessandro Stolfo and Truong An Nguyen and Mike Zhang and Yotam Perlitz and Jose Hernandez-Orallo and Runjia Li and Amin Shabani and Felix Juefei-Xu and Shikhar Dhingra and Orr Zohar and My Chiffon Nguyen and Alexander Pondaven and Abdurrahim Yilmaz and Xuandong Zhao and Chuanyang Jin and Muyan Jiang and Stefan Todoran and Xinyao Han and Jules Kreuer and Brian Rabern and Anna Plassart and Martino Maggetti and Luther Yap and Robert Geirhos and Jonathon Kean and Dingsu Wang and Sina Mollaei and Chenkai Sun and Yifan Yin and Shiqi Wang and Rui Li and Yaowen Chang and Anjiang Wei and Alice Bizeul and Xiaohan Wang and Alexandre Oliveira Arrais and Kushin Mukherjee and Jorge Chamorro-Padial and Jiachen Liu and Xingyu Qu and Junyi Guan and Adam Bouyamourn and Shuyu Wu and Martyna Plomecka and Junda Chen and Mengze Tang and Jiaqi Deng and Shreyas Subramanian and Haocheng Xi and Haoxuan Chen and Weizhi Zhang and Yinuo Ren and Haoqin Tu and Sejong Kim and Yushun Chen and Sara Vera Marjanović and Junwoo Ha and Grzegorz Luczyna and Jeff J. Ma and Zewen Shen and Dawn Song and Cedegao E. Zhang and Zhun Wang and Gaël Gendron and Yunze Xiao and Leo Smucker and Erica Weng and Kwok Hao Lee and Zhe Ye and Stefano Ermon and Ignacio D. Lopez-Miguel and Theo Knights and Anthony Gitter and Namkyu Park and Boyi Wei and Hongzheng Chen and Kunal Pai and Ahmed Elkhanany and Han Lin and Philipp D. Siedler and Jichao Fang and Ritwik Mishra and Károly Zsolnai-Fehér and Xilin Jiang and Shadab Khan and Jun Yuan and Rishab Kumar Jain and Xi Lin and Mike Peterson and Zhe Wang and Aditya Malusare and Maosen Tang and Isha Gupta and Ivan Fosin and Timothy Kang and Barbara Dworakowska and Kazuki Matsumoto and Guangyao Zheng and Gerben Sewuster and Jorge Pretel Villanueva and Ivan Rannev and Igor Chernyavsky and Jiale Chen and Deepayan Banik and Ben Racz and Wenchao Dong and Jianxin Wang and Laila Bashmal and Duarte V. Gonçalves and Wei Hu and Kaushik Bar and Ondrej Bohdal and Atharv Singh Patlan and Shehzaad Dhuliawala and Caroline Geirhos and Julien Wist and Yuval Kansal and Bingsen Chen and Kutay Tire and Atak Talay Yücel and Brandon Christof and Veerupaksh Singla and Zijian Song and Sanxing Chen and Jiaxin Ge and Kaustubh Ponkshe and Isaac Park and Tianneng Shi and Martin Q. Ma and Joshua Mak and Sherwin Lai and Antoine Moulin and Zhuo Cheng and Zhanda Zhu and Ziyi Zhang and Vaidehi Patil and Ketan Jha and Qiutong Men and Jiaxuan Wu and Tianchi Zhang and Bruno Hebling Vieira and Alham Fikri Aji and Jae-Won Chung and Mohammed Mahfoud and Ha Thi Hoang and Marc Sperzel and Wei Hao and Kristof Meding and Sihan Xu and Vassilis Kostakos and Davide Manini and Yueying Liu and Christopher Toukmaji and Jay Paek and Eunmi Yu and Arif Engin Demircali and Zhiyi Sun and Ivan Dewerpe and Hongsen Qin and Roman Pflugfelder and James Bailey and Johnathan Morris and Ville Heilala and Sybille Rosset and Zishun Yu and Peter E. Chen and Woongyeong Yeo and Eeshaan Jain and Ryan Yang and Sreekar Chigurupati and Julia Chernyavsky and Sai Prajwal Reddy and Subhashini Venugopalan and Hunar Batra and Core Francisco Park and Hieu Tran and Guilherme Maximiano and Genghan Zhang and Yizhuo Liang and Hu Shiyu and Rongwu Xu and Rui Pan and Siddharth Suresh and Ziqi Liu and Samaksh Gulati and Songyang Zhang and Peter Turchin and Christopher W. Bartlett and Christopher R. Scotese and Phuong M. Cao and Aakaash Nattanmai and Gordon McKellips and Anish Cheraku and Asim Suhail and Ethan Luo and Marvin Deng and Jason Luo and Ashley Zhang and Kavin Jindel and Jay Paek and Kasper Halevy and Allen Baranov and Michael Liu and Advaith Avadhanam and David Zhang and Vincent Cheng and Brad Ma and Evan Fu and Liam Do and Joshua Lass and Hubert Yang and Surya Sunkari and Vishruth Bharath and Violet Ai and James Leung and Rishit Agrawal and Alan Zhou and Kevin Chen and Tejas Kalpathi and Ziqi Xu and Gavin Wang and Tyler Xiao and Erik Maung and Sam Lee and Ryan Yang and Roy Yue and Ben Zhao and Julia Yoon and Sunny Sun and Aryan Singh and Ethan Luo and Clark Peng and Tyler Osbey and Taozhi Wang and Daryl Echeazu and Hubert Yang and Timothy Wu and Spandan Patel and Vidhi Kulkarni and Vijaykaarti Sundarapandiyan and Ashley Zhang and Andrew Le and Zafir Nasim and Srikar Yalam and Ritesh Kasamsetty and Soham Samal and Hubert Yang and David Sun and Nihar Shah and Abhijeet Saha and Alex Zhang and Leon Nguyen and Laasya Nagumalli and Kaixin Wang and Alan Zhou and Aidan Wu and Jason Luo and Anwith Telluri and Summer Yue and Alexandr Wang and Dan Hendrycks&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;na&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;{2025}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;na&#34;&gt;eprint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;{2501.14249}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;na&#34;&gt;archivePrefix&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;{arXiv}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;na&#34;&gt;primaryClass&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;{cs.LG}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;na&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;{https://arxiv.org/abs/2501.14249}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models</title>
      <link>https://otaheri.github.io/publication/2025_nil/</link>
      <pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2025_nil/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Acquiring physically plausible motor skills across diverse and unconventional morphologies—from humanoids to ants—is crucial for robotics and simulation.&lt;br&gt;
We introduce &lt;strong&gt;No-data Imitation Learning (NIL)&lt;/strong&gt;, which:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Generates&lt;/strong&gt; a reference video with a pretrained video diffusion model from a single simulation frame + text prompt.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learns&lt;/strong&gt; a policy in simulation by comparing rendered agent videos against the generated reference via video-encoder embeddings and segmentation-mask IoU.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;NIL matches or outperforms baselines trained on real motion‐capture data, effectively replacing expensive data collection with generative video priors.&lt;/p&gt;
&lt;h1 id=&#34;teaser&#34;&gt;Teaser&lt;/h1&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;featured.jpg&#34; class=&#34;center&#34; alt=&#34;NIL overview figure&#34; /&gt;
&lt;/p&gt;
&lt;div style=&#34;text-align:center; margin-top:10px;&#34;&gt;
  &lt;strong&gt;NIL&lt;/strong&gt; generates expert videos on-the-fly via a pretrained video diffusion model  
  and then trains policies purely from those generated 2D videos—no human data required.
&lt;/div&gt;
&lt;h1 id=&#34;nil-overview&#34;&gt;NIL Overview&lt;/h1&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;videos/method_overview.png&#34; class=&#34;center&#34; alt=&#34;Method overview diagram&#34; /&gt;
&lt;/p&gt;
&lt;div style=&#34;text-align:center; margin-top:10px;&#34;&gt;
  **Stage 1:** Generate reference video Fᵢⱼ with diffusion model D from initial frame e₀ and prompt pᵢⱼ.&lt;br&gt;
  **Stage 2:** Train policy πᵢⱼ in physics simulator to imitate Fᵢⱼ using (1) video‐encoder similarity, (2) segmentation IoU, (3) smoothness regularization.
&lt;/div&gt;
&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;
&lt;p&gt;We validate NIL on locomotion tasks for multiple morphologies (humanoids, quadrupeds, animals).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reward components:&lt;/strong&gt; ablation of video vs. mask vs. reg.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Policy performance:&lt;/strong&gt; matches or exceeds motion-capture-trained baselines.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generalization:&lt;/strong&gt; works zero-shot on unseen morphologies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;bibtex&#34;&gt;BibTeX&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nc&#34;&gt;@article&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;albaba2025nil&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;title&lt;/span&gt;   &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;author&lt;/span&gt;  &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Albaba, Mert and Li, Chenhao and Diomataris, Markos and Taheri, Omid and Krause, Andreas and Black, Michael}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;journal&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{arXiv}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;year&lt;/span&gt;    &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{2025}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>HaPTIC: Predicting 4D Hand Trajectory from Monocular Videos</title>
      <link>https://otaheri.github.io/publication/2025_haptic/</link>
      <pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2025_haptic/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;We present &lt;em&gt;HaPTIC&lt;/em&gt;, an approach that infers coherent 4D hand trajectories from monocular videos. Current video-based hand pose reconstruction methods primarily focus on improving frame-wise 3D pose using adjacent frames rather than studying consistent 4D hand trajectories in space. Despite the additional temporal cues, they generally underperform compared to image-based methods due to the scarcity of annotated video data. To address these issues, we repurpose a state-of-the-art image-based transformer to take in multiple frames and directly predict a coherent trajectory. We introduce two types of lightweight attention layers: cross-view self-attention to fuse temporal information, and global cross-attention to bring in larger spatial context. Our method infers 4D hand trajectories similar to the ground truth while maintaining strong 2D reprojection alignment. We apply the method to both egocentric and allocentric videos. It significantly outperforms existing methods in global trajectory accuracy while being comparable to the state-of-the-art in single-image pose estimation.&lt;/p&gt;
&lt;h1 id=&#34;teaser&#34;&gt;Teaser&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
  &lt;video playsinline controls autoplay muted loop style=&#34;width: 100%; display: block;&#34;&gt;
    &lt;source src=&#34;videos/teaser_blank.m4v&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
&lt;h1 id=&#34;method-overview&#34;&gt;Method Overview&lt;/h1&gt;
&lt;img src=&#34;videos/method.png&#34; class=&#34;center&#34; alt=&#34;HaPTIC Method Overview&#34; /&gt;
&lt;p&gt;&lt;strong&gt;Overall pipeline (left)&lt;/strong&gt;: HaPTIC extends HaMeR by predicting both per-frame MANO parameters and a global 4D trajectory via a shared transformer.&lt;br&gt;
&lt;strong&gt;Inside one image tower (right)&lt;/strong&gt;: We inject cross-view self-attention to fuse temporal frames and global cross-attention for spatial context.&lt;/p&gt;
&lt;h1 id=&#34;comparison-with-baselines&#34;&gt;Comparison with Baselines&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
  &lt;video controls autoplay muted loop width=&#34;100%&#34;&gt;
    &lt;source src=&#34;videos/compare.m4v&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
&lt;p&gt;The de-facto “lifting” methods suffer from jitter; metric‐depth estimators struggle under occlusion; whole-body models fail when large parts of the hand are out of view. HaPTIC delivers smooth, globally-consistent trajectories while matching 2D reprojection quality.&lt;/p&gt;
&lt;h1 id=&#34;test-time-optimization&#34;&gt;Test-Time Optimization&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
  &lt;video controls autoplay muted loop width=&#34;100%&#34;&gt;
    &lt;source src=&#34;videos/opt_crop.m4v&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
&lt;p&gt;Can a simple test-time refinement smooth out jagged feed-forward predictions? We find that while optimization can reduce local jitter, it’s much less effective at correcting global drift—HaPTIC provides a superior initialization.&lt;/p&gt;
&lt;h1 id=&#34;more-results&#34;&gt;More Results&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
  &lt;video controls autoplay muted loop width=&#34;100%&#34;&gt;
    &lt;source src=&#34;videos/itw.m4v&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
&lt;h1 id=&#34;bibtex&#34;&gt;BibTeX&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nc&#34;&gt;@article&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;ye2025predicting&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;author&lt;/span&gt;    &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Ye, Yufei and Feng, Yao and Taheri, Omid and Feng, Haiwen and Tulsiani, Shubham and Black, Michael J.}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;title&lt;/span&gt;     &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Predicting 4D Hand Trajectory from Monocular Videos}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;journal&lt;/span&gt;   &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{arXiv preprint arXiv:2501.08329}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;year&lt;/span&gt;      &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{2025}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;doi&lt;/span&gt;       &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{10.48550/arXiv.2501.08329}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>CHOIR: A Versatile and Differentiable Hand-Object Interaction Representation</title>
      <link>https://otaheri.github.io/publication/2025_choir/</link>
      <pubDate>Thu, 12 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2025_choir/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Synthesizing accurate hand–object interactions (HOI) is critical for AR/VR and vision tasks.&lt;br&gt;
Existing dense–correspondence methods improve contact fidelity but lack full differentiability or generality.&lt;br&gt;
We propose &lt;strong&gt;CHOIR&lt;/strong&gt;, a versatile, fully differentiable interaction field:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Unsigned distance fields&lt;/strong&gt; encode hand &amp;amp; object shapes continuously.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gaussian contact maps&lt;/strong&gt; capture dense hand-centric contact distributions with few parameters.&lt;br&gt;
We integrate CHOIR into &lt;strong&gt;JointDiffusion&lt;/strong&gt;, a diffusion model that learns CHOIR distributions for both:&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Refinement:&lt;/strong&gt; improves noisy reconstructions (contact F1 ↑ 5%).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Synthesis:&lt;/strong&gt; generates grasps from object geometry alone (sim. displacement ↓ 46%).&lt;br&gt;
JointDiffusion+CHOIR outperforms SOTA on refinement and synthesis benchmarks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;teaser&#34;&gt;Teaser&lt;/h1&gt;
&lt;video id=&#34;teaser&#34; autoplay muted loop playsinline height=&#34;300&#34;&gt;
  &lt;source src=&#34;videos/CHOIR_teaser.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p style=&#34;text-align:center; margin-top:10px;&#34;&gt;
  &lt;strong&gt;CHOIR&lt;/strong&gt; jointly represents hand and object geometries via unsigned distance fields,  
  and captures hand-centric contact distributions with 3D Gaussians—enabling a fully differentiable  
  and versatile hand-object interaction model.
&lt;/p&gt;
&lt;h1 id=&#34;results-carousel&#34;&gt;Results Carousel&lt;/h1&gt;
&lt;div style=&#34;display:flex; flex-wrap:wrap; gap:10px; justify-content:center;&#34;&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/scissors_0.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Scissors&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/mug_4.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Mug&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/knife_0.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Knife&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/apple_0.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Apple&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/banana_0.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Banana&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/binoculars_2.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Binoculars&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/bowl_0.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Bowl&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/camera_1.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Camera&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cell_phone_2.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Cell Phone&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cup_2.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Cup&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/eyeglasses_2.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Eyeglasses&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;method-overview&#34;&gt;Method Overview&lt;/h1&gt;
&lt;img src=&#34;videos/joint_diff_v3.png&#34; class=&#34;center&#34; alt=&#34;JointDiffusion Architecture&#34; /&gt;
&lt;h1 id=&#34;static-grasp-denoising&#34;&gt;Static Grasp Denoising&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
  &lt;h3&gt;Comparison: Denoising on Perturbed ContactPose&lt;/h3&gt;
  &lt;video controls autoplay muted loop width=&#34;400&#34;&gt;
    &lt;source src=&#34;videos/denoising.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/div&gt;
&lt;h1 id=&#34;static-grasp-synthesis&#34;&gt;Static Grasp Synthesis&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Reuses the above carousel of 11 object-specific grasp synthesis videos.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;bibtex&#34;&gt;BibTeX&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nc&#34;&gt;@article&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;morales2024choir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;author&lt;/span&gt;  &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Morales, Théo and Taheri, Omid and Lacey, Gerard}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;title&lt;/span&gt;   &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{A Versatile and Differentiable Hand-Object Interaction Representation}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;journal&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Winter Conference on Applications of Computer Vision (WACV)}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;year&lt;/span&gt;    &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{2025}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>CWGrasp: 3D Whole-Body Grasp Synthesis with Directional Controllability</title>
      <link>https://otaheri.github.io/publication/2025_cwgrasp/</link>
      <pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2025_cwgrasp/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Synthesizing 3D whole bodies that realistically grasp objects is crucial for animation, mixed reality, and robotics.&lt;br&gt;
Key challenges include natural coordination between hand, body, and environment, and the scarcity of training data.&lt;br&gt;
CWGrasp overcomes these by performing &lt;strong&gt;geometry-based reasoning early&lt;/strong&gt;: we sample a &lt;em&gt;ReachingField&lt;/em&gt; (a probabilistic
direction field around the object), then condition both the grasping hand (&lt;em&gt;CGrasp&lt;/em&gt;) and the reaching body (&lt;em&gt;CReach&lt;/em&gt;)
on this direction, and finally run a lightweight optimization to resolve penetrations.&lt;br&gt;
CWGrasp handles both left- and right-hand grasps, runs ~16× faster than exhaustive baselines (e.g., FLEX),
and outperforms them on GRAB and ReplicaGrasp datasets.&lt;br&gt;
Code and models: &lt;a href=&#34;https://gpaschalidis.github.io/cwgrasp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gpaschalidis.github.io/cwgrasp&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;teaser&#34;&gt;Teaser&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
  &lt;h2&gt;3DV 2025&lt;/h2&gt;
  &lt;img src=&#34;videos/teaser.png&#34; class=&#34;center&#34; alt=&#34;CWGrasp Teaser&#34; /&gt;
  &lt;p style=&#34;margin-top:10px;&#34;&gt;
    We develop **CWGrasp**, a framework for synthesizing 3D whole-body grasps on objects placed on receptacles.  
    By integrating early geometry-based reasoning (&lt;em&gt;ReachingField&lt;/em&gt;) with controllable synthesis, we achieve realistic 
    grasps at a fraction of the cost compared to prior art.
  &lt;/p&gt;
&lt;/div&gt;
&lt;h1 id=&#34;method-overview&#34;&gt;Method Overview&lt;/h1&gt;
&lt;img src=&#34;videos/method_overview.png&#34; class=&#34;center&#34; alt=&#34;Method Overview&#34; /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;ReachingField:&lt;/strong&gt; Ray-cast from the object to build a probabilistic direction field.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CGrasp &amp;amp; CReach:&lt;/strong&gt; Generate a hand and a body guided by the sampled direction.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimization:&lt;/strong&gt; Resolve penetrations with a fast joint refinement.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;reachingfield&#34;&gt;ReachingField&lt;/h1&gt;
&lt;div style=&#34;display:flex; gap:10px; justify-content:center; flex-wrap: wrap;&#34;&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/reachingfield_low.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Low-height&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/reachingfield_medium.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Medium-height&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/reachingfield_high.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;High-height&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;cgrasp--creach&#34;&gt;CGrasp &amp;amp; CReach&lt;/h1&gt;
&lt;div style=&#34;display:flex; gap:20px; justify-content:center;&#34;&gt;
  &lt;div style=&#34;flex:1; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/cgrasp_controllability.png&#34; alt=&#34;CGrasp&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;CGrasp:&lt;/strong&gt; Controllable hand synthesis following the direction.&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1; text-align:center;&#34;&gt;
    &lt;img src=&#34;videos/creach_controllability.png&#34; alt=&#34;CReach&#34; style=&#34;width:100%; border-radius:10px;&#34; /&gt;
    &lt;p&gt;&lt;strong&gt;CReach:&lt;/strong&gt; Directional body reaching synthesis.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;optimization-comparison&#34;&gt;Optimization Comparison&lt;/h1&gt;
&lt;div style=&#34;display:flex; gap:10px; justify-content:center; flex-wrap: wrap;&#34;&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cwgrasp_optimization.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;CWGrasp Optimization&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/flex_optimization.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;FLEX Optimization&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;qualitative-results-right-hand&#34;&gt;Qualitative Results: Right-Hand&lt;/h1&gt;
&lt;h2 id=&#34;low-height-object&#34;&gt;Low-height Object&lt;/h2&gt;
&lt;div style=&#34;display:flex; gap:20px; justify-content:center;&#34;&gt;
  &lt;div style=&#34;flex:1; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cwgrasp_camera.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;CWGrasp&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/flex_camera.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;FLEX&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;medium-height-object&#34;&gt;Medium-height Object&lt;/h2&gt;
&lt;div style=&#34;display:flex; gap:20px; justify-content:center;&#34;&gt;
  &lt;div style=&#34;flex:1; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cwgrasp_wineglass.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;CWGrasp&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/flex_wineglass.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;FLEX&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;high-height-object&#34;&gt;High-height Object&lt;/h2&gt;
&lt;div style=&#34;display:flex; gap:20px; justify-content:center;&#34;&gt;
  &lt;div style=&#34;flex:1; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cwgrasp_elephant.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;CWGrasp&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/flex_elephant.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;FLEX&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;qualitative-results-left-hand&#34;&gt;Qualitative Results: Left-Hand&lt;/h1&gt;
&lt;div style=&#34;display:flex; gap:20px; justify-content:center; flex-wrap: wrap;&#34;&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cwgrasp_left_binoculars.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Binoculars&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cwgrasp_left_camera.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Camera&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cwgrasp_left_hammer.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Hammer&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cwgrasp_left_lightbulb.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Lightbulb&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex:1 1 200px; text-align:center;&#34;&gt;
    &lt;video controls autoplay muted loop width=&#34;200&#34;&gt;
      &lt;source src=&#34;videos/cwgrasp_left_wineglass.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;p&gt;&lt;strong&gt;Wineglass&lt;/strong&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;bibtex&#34;&gt;BibTeX&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bibtex&#34; data-lang=&#34;bibtex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nc&#34;&gt;@inproceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nl&#34;&gt;paschalidis2025cwgrasp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;title&lt;/span&gt;     &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{{3D} Whole-Body Grasp Synthesis with Directional Controllability}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;author&lt;/span&gt;    &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{Paschalidis, Georgios and Wilschut, Romana and Antić, Dimitrije and Taheri, Omid and Tzionas, Dimitrios}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;booktitle&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{International Conference on 3D Vision (3DV)}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;na&#34;&gt;year&lt;/span&gt;      &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;{2025}&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>HUMOS: Human Motion Model Conditioned on Body Shape</title>
      <link>https://otaheri.github.io/publication/2024_humos/</link>
      <pubDate>Mon, 15 Jul 2024 12:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2024_humos/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Generating realistic human motion is crucial for many computer vision and graphics applications. The rich diversity of human body shapes and sizes significantly influences how people move. However, existing motion models typically overlook these differences, using a normalized, average body instead. This results in a homogenization of motion across human bodies, with motions not aligning with their physical attributes, thus limiting diversity. To address this, we propose a novel approach to learn a generative motion model conditioned on body shape. We demonstrate that it is possible to learn such a model from unpaired training data using cycle consistency, intuitive physics, and stability constraints that model the correlation between identity and movement.&lt;/p&gt;
&lt;p&gt;The resulting model, HUMOS, generates natural, physically plausible, and dynamically stable human motions conditioned on body shape. More details are available on our &lt;a href=&#34;https://github.com/CarstenEpic/humos&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;*Work done during an internship at Epic Games&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Internship Opportunity in Our Research Group</title>
      <link>https://otaheri.github.io/positions/ps_intern_videogen/</link>
      <pubDate>Mon, 15 Jul 2024 12:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/positions/ps_intern_videogen/</guid>
      <description>&lt;!-- ## Internship Opportunity --&gt;
&lt;p&gt;We are currently seeking passionate and dedicated interns to join our esteemed research group. This is a unique opportunity to work on pioneering research projects and gain invaluable experience in cutting-edge technologies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Areas of Research:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Human Motion Synthesis&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Object Interaction&lt;/li&gt;
&lt;li&gt;Scene Interaction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reconstruction from Videos/Images&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Humand and Objects&lt;/li&gt;
&lt;li&gt;Using Video Datasets for 3D Synthesis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video/3D Generation&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3D Representations&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Collecting Rich Datasets&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LLMs for Above Topics&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are looking for candidates who have experience and a strong interest in these areas, preferably with papers in top-tier conferences such as CVPR, ICCV, ECCV, ICML, NeurIPS, ICLR, and others.&lt;/p&gt;
&lt;p&gt;This is an excellent opportunity to collaborate with leading experts and contribute to groundbreaking research. If you are interested, please &lt;a href=&#34;mailto:omid.taheri@tue.mpg.de&#34;&gt;contact me&lt;/a&gt;.&lt;/p&gt;
&lt;!-- ![Internship](path/to/your/image.jpg) Replace with the actual path to the image --&gt;
</description>
    </item>
    
    <item>
      <title>PhD Defense Completed - check the photo gallery!</title>
      <link>https://otaheri.github.io/news/phd_finished/phd-finished/</link>
      <pubDate>Thu, 04 Jul 2024 12:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/news/phd_finished/phd-finished/</guid>
      <description>&lt;p&gt;I have successfully defended my PhD thesis on July 4th, 2024. It was an incredible journey, and I am grateful for the support and guidance from my advisors, committee members, and colleagues.&lt;/p&gt;
&lt;!-- The committee highly complimented my presentation slides, which were meticulously crafted to convey the essence of my research effectively. --&gt;
&lt;!-- ## Photo Album --&gt;
&lt;!-- 


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Photo 1&#34; srcset=&#34;
               /media/defence2_hue0f65e555f4d1dbd84579eed36edf79a_2268970_31c3236b9a25892d82cf83a5da37e8de.webp 400w,
               /media/defence2_hue0f65e555f4d1dbd84579eed36edf79a_2268970_5d84538101937adadb9c5c1d306d858e.webp 760w,
               /media/defence2_hue0f65e555f4d1dbd84579eed36edf79a_2268970_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://otaheri.github.io/media/defence2_hue0f65e555f4d1dbd84579eed36edf79a_2268970_31c3236b9a25892d82cf83a5da37e8de.webp&#34;
               width=&#34;466&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Photo 2&#34; srcset=&#34;
               /media/defence7_hu46529b2bea3e4e52910fefea88216f9d_2433596_6f8b619ca7366e17bc22f0d3ee8c27c4.webp 400w,
               /media/defence7_hu46529b2bea3e4e52910fefea88216f9d_2433596_4d5f0dad3a38fd7fc78fe809f450802d.webp 760w,
               /media/defence7_hu46529b2bea3e4e52910fefea88216f9d_2433596_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://otaheri.github.io/media/defence7_hu46529b2bea3e4e52910fefea88216f9d_2433596_6f8b619ca7366e17bc22f0d3ee8c27c4.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Photo 3&#34; srcset=&#34;
               /media/defence9_hu63ca05ccf36c2a7edb2bda2ed86596be_2323404_80eef77ad426bd8852a323a2fb3aa46a.webp 400w,
               /media/defence9_hu63ca05ccf36c2a7edb2bda2ed86596be_2323404_f5a461dc8992caa36d561f344f80e98f.webp 760w,
               /media/defence9_hu63ca05ccf36c2a7edb2bda2ed86596be_2323404_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://otaheri.github.io/media/defence9_hu63ca05ccf36c2a7edb2bda2ed86596be_2323404_80eef77ad426bd8852a323a2fb3aa46a.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Photo 4&#34; srcset=&#34;
               /media/defence4_hu4f9d416e2a6acf1a9dadbe06f75ae783_2581414_01f11f778044db18cade281f71051fa3.webp 400w,
               /media/defence4_hu4f9d416e2a6acf1a9dadbe06f75ae783_2581414_f408b7bc48cffa1b04be5085e3f42aa1.webp 760w,
               /media/defence4_hu4f9d416e2a6acf1a9dadbe06f75ae783_2581414_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://otaheri.github.io/media/defence4_hu4f9d416e2a6acf1a9dadbe06f75ae783_2581414_01f11f778044db18cade281f71051fa3.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
 --&gt;
&lt;!-- | ![defence2](defence2.jpg) | ![defence7](defence7.jpg) |
|:---:|:---:|
| ![defence9](defence9.jpg) | ![defence4](defence4.jpg) | --&gt;
&lt;!-- ## Watch My Defense --&gt;
&lt;div style=&#34;text-align: center; margin: 20px 0;&#34;&gt;
  &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/YOUR_VIDEO_ID&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;!-- ## Presentation and Slides --&gt;
&lt;div style=&#34;text-align: center; margin: 20px 0;&#34;&gt;
  &lt;a href=&#34;path/to/phd_defense_slides.pdf&#34; class=&#34;button&#34; style=&#34;background-color: #007BFF; color: white; padding: 10px 20px; border-radius: 5px; text-decoration: none; margin-right: 10px;&#34;&gt;View Slides (Coming Soon)&lt;/a&gt;
  &lt;a href=&#34;path/to/thesis.pdf&#34; class=&#34;button&#34; style=&#34;background-color: #17A2B8; color: white; padding: 10px 20px; border-radius: 5px; text-decoration: none;&#34;&gt;Read Thesis (Coming Soon)&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;I am excited to embark on the next chapter of my career and continue contributing to the field. Thank you to everyone who has supported me along this journey.&lt;/p&gt;
&lt;p&gt;Here are some memorable moments from my PhD defense day:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;

















&lt;div class=&#34;gallery-grid&#34;&gt;

  
  
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-./phd_defense&#34; href=&#34;https://otaheri.github.io/media/albums/phd_defense/2.jpg&#34; &gt;
      &lt;img src=&#34;https://otaheri.github.io/media/albums/phd_defense/2_hue0f65e555f4d1dbd84579eed36edf79a_2268970_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;2.jpg&#34; width=&#34;459&#34; height=&#34;750&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-./phd_defense&#34; href=&#34;https://otaheri.github.io/media/albums/phd_defense/3.jpg&#34; &gt;
      &lt;img src=&#34;https://otaheri.github.io/media/albums/phd_defense/3_hu1bfa0d313c62c3c0ebb453f87ca1088b_2984619_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;3.jpg&#34; width=&#34;750&#34; height=&#34;422&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-./phd_defense&#34; href=&#34;https://otaheri.github.io/media/albums/phd_defense/4.jpg&#34; &gt;
      &lt;img src=&#34;https://otaheri.github.io/media/albums/phd_defense/4_hu4f9d416e2a6acf1a9dadbe06f75ae783_2581414_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;4.jpg&#34; width=&#34;750&#34; height=&#34;422&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-./phd_defense&#34; href=&#34;https://otaheri.github.io/media/albums/phd_defense/5.jpg&#34; &gt;
      &lt;img src=&#34;https://otaheri.github.io/media/albums/phd_defense/5_hu0ab352c47597929766ab1623809ad0bd_3125319_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;5.jpg&#34; width=&#34;750&#34; height=&#34;422&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-./phd_defense&#34; href=&#34;https://otaheri.github.io/media/albums/phd_defense/6.jpg&#34; &gt;
      &lt;img src=&#34;https://otaheri.github.io/media/albums/phd_defense/6_hu75cc63507b5b1ece0ed13f95c37f055b_1033573_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;6.jpg&#34; width=&#34;750&#34; height=&#34;572&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-./phd_defense&#34; href=&#34;https://otaheri.github.io/media/albums/phd_defense/7.jpg&#34; &gt;
      &lt;img src=&#34;https://otaheri.github.io/media/albums/phd_defense/7_hu46529b2bea3e4e52910fefea88216f9d_2433596_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;7.jpg&#34; width=&#34;750&#34; height=&#34;422&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-./phd_defense&#34; href=&#34;https://otaheri.github.io/media/albums/phd_defense/8.jpg&#34; &gt;
      &lt;img src=&#34;https://otaheri.github.io/media/albums/phd_defense/8_hu4ce6e1de9be2ed64a972e622ffd2daae_1864533_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;8.jpg&#34; width=&#34;750&#34; height=&#34;563&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  
    
    
    
    
    
  
  
  &lt;div class=&#34;gallery-item gallery-item--medium&#34;&gt;
    &lt;a data-fancybox=&#34;gallery-./phd_defense&#34; href=&#34;https://otaheri.github.io/media/albums/phd_defense/9.jpg&#34; &gt;
      &lt;img src=&#34;https://otaheri.github.io/media/albums/phd_defense/9_hu63ca05ccf36c2a7edb2bda2ed86596be_2323404_750x750_fit_q75_h2_lanczos.webp&#34; loading=&#34;lazy&#34; alt=&#34;9.jpg&#34; width=&#34;750&#34; height=&#34;422&#34;&gt;
    &lt;/a&gt;
  &lt;/div&gt;
  

&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>WANDR: Intention-guided Human Motion Generation</title>
      <link>https://otaheri.github.io/publication/2024_wandr/</link>
      <pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2024_wandr/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness. A primary obstacle is the scarcity of training data that combines locomotion with goal reaching.&lt;/p&gt;
&lt;p&gt;To address this, we introduce WANDR, a data-driven model that takes an avatar’s initial pose and a goal’s 3D position and generates natural human motions that place the end effector (wrist) on the goal location. To solve this, we introduce novel intention features that drive rich goal-oriented movement.&lt;/p&gt;
&lt;p&gt;Intention guides the agent to the goal, and interactively adapts the generation to novel situations without needing to define sub-goals or the entire motion path. Crucially, intention allows training on datasets that have goal-oriented motions as well as those that do not. WANDR is a conditional Variational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE datasets. We evaluate our method extensively and demonstrate its ability to generate natural and long-term motions that reach 3D goals and generalize to unseen goal locations.&lt;/p&gt;
&lt;h1 id=&#34;what-is-wandr&#34;&gt;What is WANDR?&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;WANDR&lt;/strong&gt; is a conditional Variational AutoEncoder (c-VAE) that generates realistic motion of human avatars that navigate towards an arbitrary goal location and reach for it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; The initial pose of the avatar, the goal location, and the desired motion duration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt; A sequence of poses that guide the avatar from the initial pose to the goal location and place the wrist on it.&lt;/p&gt;
&lt;video id=&#34;teaser&#34; autoplay muted loop playsinline height=&#34;100%&#34;&gt;
  &lt;source src=&#34;videos/branching_motion.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
*Starting from the same state, WANDR generates diverse motions to reach different goal locations all around the human.*
&lt;h1 id=&#34;how-is-wandr-unique&#34;&gt;How is WANDR unique?&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;WANDR&lt;/strong&gt; is the first human motion generation model driven by an &lt;strong&gt;active feedback loop learned purely from data&lt;/strong&gt;, without any extra steps of reinforcement learning (RL).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Active closed loop guidance through intention features:&lt;/strong&gt; WANDR generates motion autoregressively (frame-by-frame). At each step, it predicts a state-delta that will progress the human to the next state. The prediction of the state-delta is conditioned on time- and goal-dependent features that we call &amp;ldquo;intention&amp;rdquo; (visualized as arrows in videos below). These features are computed at every frame and act as a feedback loop that guides the motion generation to reach the goal. For more details on the intention, please refer to section 3.2 of the paper.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Purely data-driven training:&lt;/strong&gt; Existing datasets that capture motion of humans reaching for goals, like &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Araujo_CIRCLE_Capture_in_Rich_Contextual_Environments_CVPR_2023_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CIRCLE&lt;/a&gt;, are scarce and have very small scale to enable generalization. This is why &lt;a href=&#34;https://arxiv.org/pdf/2310.04582.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL&lt;/a&gt; is a popular approach to learn similar tasks. However, RL comes with its own set of challenges such as sample complexity. Inspired by the paradigm of behavioral cloning we propose a purely data-driven approach where during training a future position of the avatar&amp;rsquo;s hand is considered as the goal. By hallucinating goals this way, we are able to combine both smaller datasets with goal annotations such as &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Araujo_CIRCLE_Capture_in_Rich_Contextual_Environments_CVPR_2023_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CIRCLE&lt;/a&gt;, as well as large scale like &lt;a href=&#34;https://amass.is.tue.mpg.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AMASS&lt;/a&gt; that have no goal labels but are essential to learning general navigational skills such as walking, turning etc.&lt;/p&gt;
&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;p&gt;Our method is based on a conditional Variational Auto-Encoder (c-VAE) that learns to model motion as a frame-by-frame generation process by auto-encoding the pose difference between two adjacent frames. The condition signal consists of the human’s current pose and dynamics along with the intention information. Intention is a function of both the current pose and the goal location and therefore actively guides the avatar during the motion generation in a closed loop manner. Through training, the c-VAE learns the distribution of potential subsequent poses conditioned on the current dynamic state of the human and its intention towards a specific goal.&lt;/p&gt;
&lt;p&gt;We train WANDR using two datasets: &lt;a href=&#34;https://amass.is.tue.mpg.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AMASS&lt;/a&gt;, which captures a wide range of motions including locomotion, and &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Araujo_CIRCLE_Capture_in_Rich_Contextual_Environments_CVPR_2023_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CIRCLE&lt;/a&gt;, which captures reaching motions. During inference, intention features are calculated based on the goal and act as a feedback loop that guides the motion generation towards the goal.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Method&#34; srcset=&#34;
               /publication/2024_wandr/videos/method_hue78d31216067689eb0e3a44cba1c6d3b_145102_49d444ef9c309a759cf8d6efbf963403.webp 400w,
               /publication/2024_wandr/videos/method_hue78d31216067689eb0e3a44cba1c6d3b_145102_7afe326129f5b23a5b4d39a59a00e73a.webp 760w,
               /publication/2024_wandr/videos/method_hue78d31216067689eb0e3a44cba1c6d3b_145102_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://otaheri.github.io/publication/2024_wandr/videos/method_hue78d31216067689eb0e3a44cba1c6d3b_145102_49d444ef9c309a759cf8d6efbf963403.webp&#34;
               width=&#34;760&#34;
               height=&#34;259&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;adapting-to-dynamic-goals-without-training-for-it&#34;&gt;Adapting to dynamic goals without training for it&lt;/h1&gt;
&lt;p&gt;Since &lt;strong&gt;WANDR&lt;/strong&gt; generates motion autoregressively, the intention features are updated at every frame. This allows the model to adapt to goals that move and change over time. Observe in the videos below how the intention features actively guide the avatar to orient itself towards the goal (orange arrow), get close to it (red arrow) and reach for it (blue arrow).&lt;/p&gt;
&lt;div style=&#34;display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;&#34;&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video poster=&#34;&#34; id=&#34;moving_goal1&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/moving_goal1.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video poster=&#34;&#34; id=&#34;multiple_goals4&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/multiple_goals4.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video poster=&#34;&#34; id=&#34;moving_goal2&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/moving_goal2.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video poster=&#34;&#34; id=&#34;multiple_goals3&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/multiple_goals3.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video poster=&#34;&#34; id=&#34;moving_goal5&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/moving_goal5.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;WANDR&lt;/strong&gt; generates motion autoregressively. This allows it to adapt to goals that move and change over time even though it has never been trained on scenarios with dynamic goals.&lt;/p&gt;
&lt;h1 id=&#34;video&#34;&gt;Video&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;iframe src=&#34;https://youtu.be/9szizM-XUCg?si=B836zQoWTI4I9s61&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript&#34; data-lang=&#34;gdscript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inproceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;diomataris2024wandr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;WANDR&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Intention&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;guided&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Human&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Motion&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Generation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;author&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Diomataris&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Markos&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Athanasiou&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Nikos&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Taheri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Omid&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Wang&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Xi&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Hilliges&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Otmar&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Black&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Michael&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;booktitle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Proceedings&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IEEE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conference&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Computer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Vision&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Pattern&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Recognition&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CVPR&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>GRIP: Generating Interaction Poses Using Spatial Cues and Latent Consistency</title>
      <link>https://otaheri.github.io/publication/2024_grip/</link>
      <pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2024_grip/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality. Prior work on capturing and modeling humans interacting with objects in 3D focuses on the body and object motion, often ignoring hand pose. In contrast, we introduce GRIP, a learning-based method that takes, as input, the 3D motion of the body and the object, and synthesizes realistic motion for both hands before, during, and after object interaction. As a preliminary step before synthesizing the hand motion, we first use a network, ANet, to denoise the arm motion. Then, we leverage the spatio-temporal relationship between the body and the object to extract novel temporal interaction cues, and use them in a two-stage inference pipeline to generate the hand motion. In the first stage, we introduce a new approach to encourage motion temporal consistency in the latent space (LTC) and generate consistent interaction motions. In the second stage, GRIP generates refined hand poses to avoid hand-object penetrations. Given sequences of noisy body and object motion, GRIP “upgrades” them to include hand-object interaction. Quantitative experiments and perceptual studies demonstrate that GRIP outperforms baseline methods and generalizes to unseen objects and motions from different motion-capture datasets. Our models and code are available for research purposes at &lt;a href=&#34;https://grip.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GRIP&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;video&#34;&gt;Video&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/IpIIQrdahYs&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h1 id=&#34;data-and-code&#34;&gt;Data and Code&lt;/h1&gt;
&lt;p&gt;Please register and accept the License Agreement on this website to access the GRIP models. When creating an account, &lt;strong&gt;please opt-in for email communication&lt;/strong&gt;, so that we can &lt;strong&gt;reach out to you&lt;/strong&gt; via email to announce potential &lt;strong&gt;significant updates&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;download.php&#34;&gt;Model files/weights&lt;/a&gt; (works &lt;em&gt;only&lt;/em&gt; after sign-in)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/otaheri/GRIP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code&lt;/a&gt; (GitHub)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;col-md-6&#34;&gt;
    &lt;video poster=&#34;&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/s3_camera_pass_1_renders_body_flat.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;div class=&#34;caption&#34;&gt;Input Motion 1&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;col-md-6&#34;&gt;
    &lt;video poster=&#34;&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/s3_camera_pass_1_renders_body_ref.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;div class=&#34;caption&#34;&gt;Output Motion 1&lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;col-md-6&#34;&gt;
    &lt;video poster=&#34;&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/s1_binoculars_offhand_1_renders_body_flat.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;div class=&#34;caption&#34;&gt;Input Motion 2&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;col-md-6&#34;&gt;
    &lt;video poster=&#34;&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/s1_binoculars_offhand_1_renders_body_ref.mp4&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;div class=&#34;caption&#34;&gt;Output Motion 2&lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;col-md-6&#34;&gt;
    &lt;video poster=&#34;&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/proximity_motion.mov&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;div class=&#34;caption&#34;&gt;Proximity Sensor&lt;/div&gt;
  &lt;/div&gt;
  &lt;div class=&#34;col-md-6&#34;&gt;
    &lt;video poster=&#34;&#34; autoplay controls muted loop playsinline height=&#34;100%&#34;&gt;
      &lt;source src=&#34;videos/rh_ambient.mov&#34; type=&#34;video/mp4&#34;&gt;
    &lt;/video&gt;
    &lt;div class=&#34;caption&#34;&gt;Ambient Sensor&lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript&#34; data-lang=&#34;gdscript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inproceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;taheri2024grip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;  &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GRIP&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Generating&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Interaction&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Poses&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Using&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Latent&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Consistency&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Spatial&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Cues&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;author&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Omid&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Taheri&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Yi&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Zhou&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dimitrios&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tzionas&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Yang&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Zhou&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Duygu&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Ceylan&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Soren&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Pirk&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Michael&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Black&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;booktitle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;International&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conference&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;D&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Vision&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DV&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;url&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;https&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;//&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;grip&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tue&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mpg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;de&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</title>
      <link>https://otaheri.github.io/publication/2024_intercap/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2024_intercap/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Humans constantly interact with objects to accomplish tasks. To understand such interactions, computers need to reconstruct these in 3D from images of whole bodies manipulating objects, e.g., for grasping, moving, and using the latter. This involves key challenges, such as occlusion between the body and objects, motion blur, depth ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community has followed a divide-and-conquer approach, focusing either only on interacting hands, ignoring the body, or on interacting bodies, ignoring the hands. However, these are only parts of the problem. On the contrary, recent work focuses on the whole problem. The GRAB dataset addresses whole-body interaction with dexterous hands but captures motion via markers and lacks video, while the BEHAVE dataset captures video of body-object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body SMPL-X model and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the body and object can be used to improve the pose estimation of both. (ii) Consumer-level Azure Kinect cameras let us set up a simple and flexible multi-view RGB-D system for reducing occlusions, with spatially calibrated and temporally synchronized cameras. With our InterCap method, we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 daily objects of various sizes and affordances, including contact with the hands or feet. To this end, we introduce a new data-driven hand motion prior, as well as explore simple ways for automatic contact detection based on 2D and 3D cues. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images, paired with pseudo ground-truth 3D body and object meshes. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are available for research purposes.&lt;/p&gt;
&lt;h1 id=&#34;video&#34;&gt;Video&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/d5wHLDIqN6c&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;Please register and accept the License agreement on &lt;a href=&#34;https://intercap.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InterCap&lt;/a&gt; website in order to get access to the dataset.&lt;/p&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript&#34; data-lang=&#34;gdscript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;article&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;huang2024intercap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;InterCap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Joint&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Markerless&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tracking&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Humans&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Objects&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Interaction&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Multi&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RGB&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Images&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;author&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Huang&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Yinghao&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Taheri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Omid&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Black&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Michael&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tzionas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dimitrios&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;journal&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;International&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Journal&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Computer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Vision&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;IJCV&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)}},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;volume&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;number&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pages&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;doi&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;10.1007&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s11263&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;024&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01984&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation</title>
      <link>https://otaheri.github.io/publication/2023_arctic/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2023_arctic/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronized motion of hands and articulated objects. To this end, we introduce ARCTIC &amp;ndash; a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively, and evaluate them qualitatively and quantitatively on ARCTIC.&lt;/p&gt;
&lt;h1 id=&#34;dexterous-motion--dynamic-contact&#34;&gt;Dexterous Motion + Dynamic Contact&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;video id=&#34;dollyzoom&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34; height=&#34;100%&#34;&gt;
  &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/dexterous.mp4&#34; type=&#34;video/mp4&#34; /&gt;
&lt;/video&gt;
&lt;/div&gt;
&lt;h1 id=&#34;annotation-with-mano&#34;&gt;Annotation with MANO&lt;/h1&gt;
&lt;div style=&#34;display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;&#34;&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;steve&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/mano/seq01.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;chair-tp&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/mano/seq02.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;shiba&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/mano/seq03.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;fullbody&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/mano/seq04.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;blueshirt&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/mano/seq05.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;mask&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/mano/seq06.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;annotation-with-smplx&#34;&gt;Annotation with SMPLX&lt;/h1&gt;
&lt;div style=&#34;display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;&#34;&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/smplx/seq01.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/smplx/seq02.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/smplx/seq04.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/smplx/seq04.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;rendered-depth&#34;&gt;Rendered Depth&lt;/h1&gt;
&lt;p&gt;Here we only visualize human + object for simplicity.&lt;/p&gt;
&lt;div style=&#34;display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;&#34;&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/depth/seq03.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/depth/seq04.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/depth/seq05.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/depth/seq06.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;steve&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/depth/seq01.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
  &lt;div style=&#34;flex: 1 1 300px;&#34;&gt;
    &lt;video id=&#34;chair-tp&#34; poster=&#34;&#34; autoplay=&#34;autoplay&#34; loop=&#34;loop&#34; muted=&#34;&#34; controls=&#34;controls&#34; width=&#34;100%&#34;&gt;
      &lt;source src=&#34;https://download.is.tue.mpg.de/arctic/static/videos/depth/seq02.mp4&#34; type=&#34;video/mp4&#34; /&gt;
    &lt;/video&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript&#34; data-lang=&#34;gdscript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inproceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fan2023arctic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ARCTIC&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dexterous&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Bimanual&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Hand&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Object&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Manipulation&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;author&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Fan&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Zicong&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Taheri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Omid&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tzionas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dimitrios&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Kocabas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Muhammed&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Kaufmann&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Manuel&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Black&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Michael&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Hilliges&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Otmar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;booktitle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Proceedings&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;IEEE&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conference&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Computer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Vision&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Pattern&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Recognition&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CVPR&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2023&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>IPMAN: 3D Human Pose Estimation via Intuitive Physics</title>
      <link>https://otaheri.github.io/publication/2023_ipman/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2023_ipman/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body’s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a “stable” configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH, and Human3.6M shows that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research.&lt;/p&gt;
&lt;h1 id=&#34;video&#34;&gt;Video&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;iframe src=&#34;https://www.youtube.com/embed/Dufvp_O0ziU&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h1 id=&#34;data-and-code&#34;&gt;Data and Code&lt;/h1&gt;
&lt;p&gt;Please register and accept the License agreement on the &lt;a href=&#34;https://moyo.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MoYo&lt;/a&gt; website in order to get access to the dataset.&lt;/p&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript&#34; data-lang=&#34;gdscript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inproceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tripathi2023ipman&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Human&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Pose&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Estimation&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;via&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Intuitive&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Physics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;author&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Tripathi&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Shashank&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;M&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;\&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;u}ller, Lea and Huang, Chun-Hao P. and Taheri Omid and Black, Michael J. and Tzionas, Dimitrios},
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    booktitle = &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{Conference on Computer Vision and Pattern Recognition ({CVPR})}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    pages = &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{4713--4725}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    year = &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{2023}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    url = &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{https://ipman.is.tue.mpg.de}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</title>
      <link>https://otaheri.github.io/publication/2022_intercap/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2022_intercap/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Humans constantly interact with objects to accomplish tasks. To understand such interactions, computers need to reconstruct these in 3D from images of whole bodies manipulating objects, e.g., for grasping, moving, and using the latter. This involves key challenges, such as occlusion between the body and objects, motion blur, depth ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community has followed a divide-and-conquer approach, focusing either only on interacting hands, ignoring the body, or on interacting bodies, ignoring the hands. However, these are only parts of the problem. On the contrary, recent work focuses on the whole problem. The GRAB dataset addresses whole-body interaction with dexterous hands but captures motion via markers and lacks video, while the BEHAVE dataset captures video of body-object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body SMPL-X model and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the body and object can be used to improve the pose estimation of both. (ii) Consumer-level Azure Kinect cameras let us set up a simple and flexible multi-view RGB-D system for reducing occlusions, with spatially calibrated and temporally synchronized cameras. With our InterCap method, we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 daily objects of various sizes and affordances, including contact with the hands or feet. To this end, we introduce a new data-driven hand motion prior, as well as explore simple ways for automatic contact detection based on 2D and 3D cues. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images, paired with pseudo ground-truth 3D body and object meshes. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are available for research purposes.&lt;/p&gt;
&lt;h1 id=&#34;video&#34;&gt;Video&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/d5wHLDIqN6c&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;Please register and accept the License agreement on &lt;a href=&#34;https://intercap.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InterCap&lt;/a&gt; website in order to get access to the dataset.&lt;/p&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript&#34; data-lang=&#34;gdscript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;article&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;huang2024intercap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;InterCap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Joint&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Markerless&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tracking&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Humans&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Objects&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Interaction&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Multi&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;view&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RGB&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Images&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;author&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Huang&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Yinghao&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Taheri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Omid&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Black&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Michael&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tzionas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dimitrios&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;journal&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;International&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Journal&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Computer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Vision&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;IJCV&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)}},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;volume&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;number&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pages&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;doi&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;10.1007&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s11263&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;024&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;01984&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inproceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;huang2022intercap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;InterCap&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;oint&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Markerless&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tracking&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Humans&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Objects&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Interaction&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;author&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Huang&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Yinghao&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Taheri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Omid&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Black&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Michael&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tzionas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dimitrios&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;booktitle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;German&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conference&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Pattern&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Recognition&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GCPR&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)}},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;volume&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;13485&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pages&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;281&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;--&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;299&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2022&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;organization&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Springer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;series&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Lecture&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Notes&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Computer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Science&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping</title>
      <link>https://otaheri.github.io/publication/2022_goal/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2022_goal/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand, and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s generated motions approach the realism of GRAB’s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research purposes.&lt;/p&gt;
&lt;h1 id=&#34;what-is-goal&#34;&gt;What is GOAL?&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;GOAL&lt;/strong&gt; is a generative model that generates full-body motion of the human body that walks and grasps unseen 3D objects. GOAL consists of two main steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;GNet&lt;/strong&gt; generates the final grasp of the motion.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MNet&lt;/strong&gt; generates the motion from the starting to the grasp frame.
It is trained on the &lt;a href=&#34;http://grab.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GRAB&lt;/a&gt; dataset. For more details please refer to the &lt;a href=&#34;https://arxiv.org/abs/2112.11454&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper&lt;/a&gt; or the &lt;a href=&#34;http://goal.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project website&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;GOAL generates diverse motions to reach different goal locations around the human.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;gnet&#34;&gt;GNet&lt;/h1&gt;
&lt;p&gt;Below you can see some generated whole-body static grasps from GNet. The hand close-ups are from the same grasp, and for better visualization:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Apple&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Binoculars&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Toothpaste&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GOAL/raw/main/images/s1_apple_rh.gif&#34; alt=&#34;Apple&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GOAL/raw/main/images/s4_binoculars_rh.gif&#34; alt=&#34;Binoculars&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GOAL/raw/main/images/s5_toothpaste_rh.gif&#34; alt=&#34;Toothpaste&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GOAL/raw/main/images/s1_apple.gif&#34; alt=&#34;Apple&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GOAL/raw/main/images/s4_binoculars.gif&#34; alt=&#34;Binoculars&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GOAL/raw/main/images/s5_toothpaste.gif&#34; alt=&#34;Toothpaste&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;mnet&#34;&gt;MNet&lt;/h1&gt;
&lt;p&gt;Below you can see some generated whole-body motions that walk and grasp 3D objects using MNet:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Camera&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Mug&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Apple&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GOAL/raw/main/images/s2_camera_lift_motion.gif&#34; alt=&#34;Camera&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GOAL/raw/main/images/s6_mug_drink_2_motion.gif&#34; alt=&#34;Mug&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GOAL/raw/main/images/s1_apple_eat_1_motion.gif&#34; alt=&#34;Apple&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For more details check out the YouTube video below.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=A7b8DYovDZY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GOAL/raw/main/images/video_teaser_play.png&#34; alt=&#34;Video&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;data-and-code&#34;&gt;Data and Code&lt;/h1&gt;
&lt;p&gt;Please register and accept the License agreement on &lt;a href=&#34;https://grab.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GRAB&lt;/a&gt; website to get access to the GRAB dataset.&lt;/p&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript&#34; data-lang=&#34;gdscript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inproceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;taheri2022goal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GOAL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;G&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;enerating&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;D&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Whole&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Body&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Motion&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Hand&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;Object&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Grasping&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;author&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Taheri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Omid&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Choutas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Vasileios&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Black&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Michael&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tzionas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dimitrios&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;booktitle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Conference&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Computer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Vision&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Pattern&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Recognition&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;CVPR&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2022&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;url&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;https&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;//&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;goal&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tue&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mpg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;de&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inproceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GRAB&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2020&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GRAB&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Whole&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Body&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Human&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Grasping&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Objects&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;author&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Taheri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Omid&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Ghorbani&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Nima&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Black&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Michael&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tzionas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dimitrios&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;booktitle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;European&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conference&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Computer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Vision&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ECCV&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;})},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2020&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;url&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;https&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;//&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;grab&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tue&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mpg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;de&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>GRAB: A Dataset of Whole-Body Human Grasping of Objects</title>
      <link>https://otaheri.github.io/publication/2020_grab/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2020_grab/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While &amp;ldquo;grasping&amp;rdquo; is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of &amp;ldquo;whole-body grasps&amp;rdquo;. Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes.&lt;/p&gt;
&lt;h1 id=&#34;tldr&#34;&gt;TL;DR&lt;/h1&gt;
&lt;p&gt;We capture a very accurate dataset, named GRAB, of people interacting with 3D objects. We then use it to train a network, GrabNet, that generates hand grasps for novel objects.&lt;/p&gt;
&lt;h1 id=&#34;grab-dataset&#34;&gt;GRAB Dataset&lt;/h1&gt;
&lt;h2 id=&#34;dataset-overview&#34;&gt;Dataset Overview&lt;/h2&gt;
&lt;p&gt;GRAB is a dataset of full-body motions interacting and grasping 3D objects. It contains accurate finger and facial motions as well as the contact between the objects and body. The dataset includes 5 male and 5 female participants performing 4 different motion intents with 51 everyday objects.&lt;/p&gt;
&lt;h3 id=&#34;example-motions&#34;&gt;Example Motions&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Eat - Banana&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Talk - Phone&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Drink - Mug&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;See - Binoculars&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GRAB/raw/master/images/banana.gif&#34; alt=&#34;Banana&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GRAB/raw/master/images/phone.gif&#34; alt=&#34;Phone&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GRAB/raw/master/images/mug.gif&#34; alt=&#34;Mug&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GRAB/raw/master/images/binoculars.gif&#34; alt=&#34;Binoculars&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The GRAB dataset also contains binary contact maps between the body and objects. With our interacting meshes, one could integrate these contact maps over time to create &amp;ldquo;contact heatmaps&amp;rdquo;, or even compute fine-grained contact annotations, as shown below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Contact Heatmaps&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Contact Annotation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GRAB/raw/master/images/contact.png&#34; alt=&#34;contact&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GRAB/raw/master/images/contact1.png&#34; alt=&#34;contact1&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;dataset-videos&#34;&gt;Dataset Videos&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Long Video&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Short Video&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://youtu.be/s5syYMxmNHA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GRAB/raw/master/images/long.png&#34; alt=&#34;LongVideo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://youtu.be/VHN0DBUB4H8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GRAB/raw/master/images/short.png&#34; alt=&#34;ShortVideo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;grabnet&#34;&gt;GrabNet&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;GrabNet is a generative model for 3D hand grasps. Given a 3D object mesh, GrabNet can predict several hand grasps for it. GrabNet has two successive models, CoarseNet (cVAE) and RefineNet. It is trained on a subset (right hand and object only) of the GRAB dataset.&lt;/p&gt;
&lt;h3 id=&#34;generated-results-from-grabnet&#34;&gt;Generated Results from GrabNet&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Binoculars&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Mug&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Camera&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Toothpaste&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GrabNet/raw/master/images/binoculars.gif&#34; alt=&#34;Binoculars&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GrabNet/raw/master/images/mug.gif&#34; alt=&#34;Mug&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GrabNet/raw/master/images/camera.gif&#34; alt=&#34;Camera&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GrabNet/raw/master/images/toothpaste.gif&#34; alt=&#34;Toothpaste&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;grabnet-videos&#34;&gt;GrabNet Videos&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Long Video&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Short Video&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://youtu.be/s5syYMxmNHA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GRAB/raw/master/images/long.png&#34; alt=&#34;LongVideo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;https://youtu.be/VHN0DBUB4H8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://github.com/otaheri/GRAB/raw/master/images/short.png&#34; alt=&#34;ShortVideo&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;data-and-code&#34;&gt;Data and Code&lt;/h1&gt;
&lt;p&gt;Please register and accept the License agreement on &lt;a href=&#34;https://grab.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; website in order to get access to the GRAB dataset. The license and downloads section include explicit restrictions per subject, to which you agree to comply with.&lt;/p&gt;
&lt;p&gt;When creating an account, please opt-in for email communication, so that we can reach out to you per email to announce potential significant updates.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://grab.is.tue.mpg.de/download.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GRAB dataset (works only after sign-in)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grab.is.tue.mpg.de/download.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GrabNet data (works only after sign-in)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grab.is.tue.mpg.de/download.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GrabNet model files/weights (works only after sign-in)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/otaheri/GRAB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code for GRAB (GitHub)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/otaheri/GrabNet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Code for GrabNet (GitHub)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript&#34; data-lang=&#34;gdscript&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;@&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inproceedings&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GRAB&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2020&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;GRAB&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dataset&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Whole&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Body&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Human&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Grasping&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Objects&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;author&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Taheri&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Omid&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Ghorbani&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Nima&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Black&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Michael&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;J&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tzionas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Dimitrios&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;booktitle&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;European&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Conference&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Computer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Vision&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ECCV&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2020&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;url&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;https&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;//&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;grab&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tue&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mpg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;de&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Human Leg Motion Tracking by Fusing IMUs and RGB Camera Data Using Extended Kalman Filter</title>
      <link>https://otaheri.github.io/publication/2019_leg_imu/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2019_leg_imu/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Human motion capture is frequently used to study rehabilitation and clinical problems, as well
as to provide realistic animation for the entertainment industry. IMU-based systems, as well as Markerbased
motion tracking systems, are most popular methods to track movement due to their low cost of
implementation and lightweight. This paper proposes a quaternion-based Extended Kalman filter
approach to recover the human leg segments motions with a set of IMU sensors data fused with
cameramarker system data. In this paper, an Extended Kalman Filter approach is developed to fuse the data of
two IMUs and one RGB camera for human leg motion tracking. Based on the complementary properties
of the inertial sensors and camera-marker system, in the introduced new measurement model, the
orientation data of the upper leg and the lower leg is updated through three measurement equations. The
positioning of the human body is made possible by the tracked position of the pelvis joint by the camera
marker system. A mathematical model has been utilized to estimate joints’ depth in 2D images. The
efficiency of the proposed algorithm is evaluated by an optical motion tracker system.&lt;/p&gt;
&lt;!--
# Video

&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;iframe src=&#34;https://download.is.tue.mpg.de/arctic/video.mp4&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;


# Data and Code

Please register and accept the License agreement on [GRAB](https://grab.is.tue.mpg.de) website in order to get access to the GRAB dataset. 



# Citation

```

```
--&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>https://otaheri.github.io/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://otaheri.github.io/privacy/</guid>
      <description>&lt;p&gt;Add your privacy policy here and set &lt;code&gt;draft: false&lt;/code&gt; to publish it. Otherwise, delete this file if you don&amp;rsquo;t need it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>https://otaheri.github.io/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://otaheri.github.io/terms/</guid>
      <description>&lt;p&gt;Add your terms here and set &lt;code&gt;draft: false&lt;/code&gt; to publish it. Otherwise, delete this file if you don&amp;rsquo;t need it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://otaheri.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
