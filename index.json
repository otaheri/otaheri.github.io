
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":["admin"],"categories":null,"content":" Hello!\nI am a Ph.D. researcher in Perceiving Systems at the MPI for Intelligent Systems, advised by Professor Michael J. Black and Dr. Dimitrios Tzionas.\nI am interested in generating Virtual Humans that move and interact with 3D scenes and objects like Real Humans.\nMy broad research view includes: precise MoCap using multimodal sensors (IMUs, Cameras, Pressure Sensors, etc), 3D reconstruction from images, Human-Scene Interaction, and 3D representation.\nFeel free to contact me about my research, collaborations, or anything else here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hello!\nI am a Ph.D. researcher in Perceiving Systems at the MPI for Intelligent Systems, advised by Professor Michael J. Black and Dr. Dimitrios Tzionas.\nI am interested in generating Virtual Humans that move and interact with 3D scenes and objects like Real Humans.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":null,"content":"\rClick on the Slides button above to view the built-in slides feature.\rSlides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://otaheri.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["**Omid Taheri**","Yi Zhou","Dimitrios Tzionas","Yang Zhou","Duygu Ceylan","Soren Pirk","Michael J. Black"],"categories":null,"content":"Abstract Hands are dexterous and highly versatile manipulators, that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality. To address this challenge, we propose GRIP, a learning-based method that synthesizes the motion of both left and right hands before, during, and after interaction with objects. We leverage the dynamics between the body and the object to extract two types of novel temporal interaction cues. We then define a two-stage inference pipeline in which GRIP first outputs the consistent interaction motion by taking a new approach to modeling motion temporal consistency in the latent space. In the second stage, GRIP generates refined hand poses to avoid finger-to-object penetrations. Both quantitative experiments and perceptual studies demonstrate that GRIP outperforms existing methods and generalizes to unseen objects and motions. Furthermore, GRIP achieves real-time performance with 45 fps hand pose prediction. Our models and code will be available for research purposes.\n","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"cc4b2fd6941d367fe8a5867780e085d7","permalink":"https://otaheri.github.io/publication/2023_grip/","publishdate":"2022-08-01T00:00:00Z","relpermalink":"/publication/2023_grip/","section":"publication","summary":"Abstract Hands are dexterous and highly versatile manipulators, that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality.","tags":[],"title":"GRIP: Generating Interaction Poses Conditioned on Object and Body Motion","type":"publication"},{"authors":["Zicong Fan","**Omid Taheri**","Dimitrios Tzionas","Muhammed Kocabas","Manuel Kaufmann","Michael J. Black","Otmar Hilliges"],"categories":null,"content":"Abstract We use our hands to interact with and to manipulate objects. Articulated objects are especially interesting since they often require the full dexterity of human hands to manipulate them. To understand, model, and synthesize such interactions, automatic and robust methods that reconstruct hands and articulated objects in 3D from a color image are needed. Existing methods for estimating 3D hand and object pose from images focus on rigid objects. In part, because such methods rely on training data and no dataset of articulated object manipulation exists. Consequently, we introduce ARCTIC – the first dataset of free-form interactions of hands and articulated objects. ARCTIC has 1.2M images paired with accurate 3D meshes for both hands and for objects that move and deform over time. The dataset also provides hand-object contact information. To show the value of our dataset, we perform two novel tasks on ARCTIC: (1) 3D reconstruction of two hands and an articulated object in interaction; (2) an estimation of dense hand-object relative distances, which we call interaction field estimation. For the first task, we present ArcticNet, a baseline method for the task of jointly reconstructing two hands and an articulated object from an RGB image. For interaction field estimation, we predict the relative distances from each hand vertex to the object surface, and vice versa. We introduce InterField, the first method that estimates such distances from a single RGB image. We provide qualitative and quantitative experiments for both tasks, and provide detailed analysis on the data.\nVideo ","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"3562e35d6d487017b16f8c99a4589e45","permalink":"https://otaheri.github.io/publication/2023_arctic/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/2023_arctic/","section":"publication","summary":"Abstract We use our hands to interact with and to manipulate objects. Articulated objects are especially interesting since they often require the full dexterity of human hands to manipulate them. To understand, model, and synthesize such interactions, automatic and robust methods that reconstruct hands and articulated objects in 3D from a color image are needed.","tags":[],"title":"ARCTIC: Articulated Objects in Free-form Hand Interaction","type":"publication"},{"authors":["**Omid Taheri**","Vasileios Choutas","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s generated motions approach the realism of GRAB’s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research.\nVideo Data and Code Please register and accept the License agreement on GRAB website in order to get access to the GRAB dataset.\nCitation @inproceedings{taheri2022goal, title = {{GOAL}: {G}enerating {4D} Whole-Body Motion for Hand-Object Grasping}, author = {Taheri, Omid and Choutas, Vasileios and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {Conference on Computer Vision and Pattern Recognition ({CVPR})}, year = {2022}, url = {https://goal.is.tue.mpg.de} } @inproceedings{GRAB:2020, title = {{GRAB}: {A} Dataset of Whole-Body Human Grasping of Objects}, author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {European Conference on Computer Vision ({ECCV})}, year = {2020}, url = {https://grab.is.tue.mpg.de} } ","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"4a7fd05aa8806e64f6254022b718ad5e","permalink":"https://otaheri.github.io/publication/goal-generating-4d-whole-body-motion-for-hand-object-grasping/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/goal-generating-4d-whole-body-motion-for-hand-object-grasping/","section":"blog","summary":"Abstract Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects.","tags":[],"title":"GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping","type":"blog"},{"authors":["Yinghao Huang","**Omid Taheri**","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are available for research purposes.\nVideo Data Please register and accept the License agreement on InterCap website in order to get access to the dataset.\nCitation @inproceedings{huang2022intercap, title = {{InterCap}: {J}oint Markerless {3D} Tracking of Humans and Objects in Interaction}, author = {Huang, Yinghao and Taheri, Omid and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {{German Conference on Pattern Recognition (GCPR)}}, volume = {13485}, pages = {281--299}, year = {2022}, organization = {Springer}, series = {Lecture Notes in Computer Science} } ","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"901fcc62162f6f671f78fb1360df5bdf","permalink":"https://otaheri.github.io/publication/2022_intercap/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022_intercap/","section":"publication","summary":"Abstract Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts.","tags":[],"title":"InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction","type":"publication"},{"authors":["Jack Ratcliffe","**Nick Ballou**","Laurissa Tokarchuk"],"categories":null,"content":"","date":1639008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639008000,"objectID":"0d0400674d84f312bdc0c2ab5c26ea85","permalink":"https://otaheri.github.io/example/2021-ratcliffe-et-al-actions/","publishdate":"2021-12-09T00:00:00Z","relpermalink":"/example/2021-ratcliffe-et-al-actions/","section":"example","summary":"Modern immersive virtual reality (IVR) often uses embodied con- trollers for interacting with virtual objects. However, it is not clear how we should conceptualise these interactions. They could be con- sidered either gestures, as there is no interaction with a physical object; or as actions, given that there is object manipulation, even if it is virtual. This distinction is important, as literature has shown that in the physical world, action-enabled and gesture-enabled learning produce distinct cognitive outcomes. This study attempts to understand whether sensorimotor-embodied interactions with objects in IVR can cognitively be considered as actions or gestures. It does this by comparing verb-learning outcomes between two conditions: (1) where participants move the controllers without touching virtual objects (gesture condition); and (2) where partici- pants move the controllers and manipulate virtual objects (action condition). We found that (1) users can have cognitively distinct outcomes in IVR based on whether the interactions are actions or gestures, with actions providing stronger memorisation outcomes; and (2) embodied controller actions in IVR behave more similarly to physical world actions in terms of verb memorization benefits.","tags":null,"title":"Actions, not gestures: contextualising embodied controller interactions in immersive virtual reality","type":"example"},{"authors":["**Nick Ballou**","Heiko Breitsohl","Dominic Kao","Kathrin Gerling","Sebastian Deterding"],"categories":null,"content":"","date":1634515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634515200,"objectID":"534111fa1d721b0d5f5d0307b385727c","permalink":"https://otaheri.github.io/example/2021-ballou-et-al-not-very-effective/","publishdate":"2021-10-18T00:00:00Z","relpermalink":"/example/2021-ballou-et-al-not-very-effective/","section":"example","summary":"Effectance—the basic positive experience of causing effects—provides a promising explanation for the enjoyment derived from novel low-challenge game genres featuring ample ‘juicy’ feedback. To date, game researchers have studied effectance using a little-validated 11-item scale developed by Klimmt, Hartmann, and Frey. To test its dimensionality and discriminant validity, we conducted an online survey (n = 467) asking people to report on effectance and related experiences in a recent play session. Confirmatory and exploratory factor analyses show poor fit with a unidimensional factor structure and poor discriminant validity with common enjoyment and mastery/competence measures, likely due to reverse-coded items and a separable input lag factor. We discuss further possible validity issues like questionable content validity, advise against using the scale in its present form, and close with recommendations for future scale development and use.","tags":null,"title":"Not Very Effective: Validity Issues of the Effectance in Games Scale","type":"example"},{"authors":["**Omid Taheri**","Vasileios Choutas","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s generated motions approach the realism of GRAB’s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research.\nVideo Data and Code Please register and accept the License agreement on GRAB website in order to get access to the GRAB dataset.\nCitation @inproceedings{taheri2022goal, title = {{GOAL}: {G}enerating {4D} Whole-Body Motion for Hand-Object Grasping}, author = {Taheri, Omid and Choutas, Vasileios and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {Conference on Computer Vision and Pattern Recognition ({CVPR})}, year = {2022}, url = {https://goal.is.tue.mpg.de} } @inproceedings{GRAB:2020, title = {{GRAB}: {A} Dataset of Whole-Body Human Grasping of Objects}, author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {European Conference on Computer Vision ({ECCV})}, year = {2020}, url = {https://grab.is.tue.mpg.de} } ","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"f0c3bdbf060cff0e3631900404375d50","permalink":"https://otaheri.github.io/publication/2022_goal/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/2022_goal/","section":"publication","summary":"Abstract Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects.","tags":[],"title":"GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping","type":"publication"},{"authors":["**Nick Ballou**","Vivek Warriar","Sebastian Deterding"],"categories":null,"content":"","date":1611878400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611878400,"objectID":"429bea50dc36a6867668c1ca8a3c75f2","permalink":"https://otaheri.github.io/example/2021-ballou-et-al-are-you-open/","publishdate":"2021-01-29T00:00:00Z","relpermalink":"/example/2021-ballou-et-al-are-you-open/","section":"example","summary":"Within the wider open science reform movement, HCI researchers are actively debating how to foster transparency in their own field. Publication venues play a crucial role in instituting open science practices, especially journals, whose procedures arguably lend themselves better to them than conferences. Yet we know lit- tle about how much HCI journals presently support open science practices. We identified the 51 most frequently published-in jour- nals by recent CHI first authors and coded them according to the Transparency and Openness Promotion guidelines, a high-profile standard of evaluating editorial practices. Results indicate that jour- nals in our sample currently do not set or specify clear openness and transparency standards. Out of a maximum of 29, the modal score was 0 (mean = 2.5, SD = 3.6, max = 15). We discuss potential reasons, the aptness of natural science-based guidelines for HCI, and next steps for the HCI community in furthering openness and transparency.","tags":null,"title":"Are You Open? A Content Analysis of Transparency and Openness Guidelines in HCI Journals","type":"example"},{"authors":["**Nick Ballou**","Charles Gbadamosi","David Zendle"],"categories":null,"content":"","date":1605139200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605139200,"objectID":"8b277f3d74f7307217386cf1938f1267","permalink":"https://otaheri.github.io/example/2020-ballou-et-al-hidden/","publishdate":"2020-11-12T00:00:00Z","relpermalink":"/example/2020-ballou-et-al-hidden/","section":"example","summary":"Loot boxes are the focus of growing research and regulatory attention. While they are frequently treated as a monolithic feature of games by researchers and policymakers, loot box implementations are not uniform: the features of loot boxes vary from game to game in ways that may have important consequences for player spending and behaviour. Despite this, previous attempts to classify loot boxes have either not focused on the impact of loot box features on player behaviour and spending, or have not attempted to fully map the different forms that loot boxes currently take. In this work, we attempt to illustrate the nuance present in loot box implementation in a featural model. Using our lived experience, a qualitative coding exercise, and consultation with an industry professional, we identify thirty-two features of loot box-like mechanics that might be expected to influence player behavior or spending, which we group into five domains: point of purchase, pulling procedure, contents, audiovisual presentation, and salience. Each feature is broken down into two or more categorization tags for a given loot box, and illustrative examples of each feature are provided. This work may serve to guide researchers in studying how different types of loot boxes may affect players, aid regulators in ensuring that any proposed legislation is sufficiently nuanced to handle the wide variation in loot box design, and help parents and players to better understand the inner workings of loot boxes during play.","tags":null,"title":"The Hidden Intricacy of Loot Box Design: A Granular Description of Random Monetized Reward Features","type":"example"},{"authors":["**Nick Ballou**","Antonius J Van Rooij"],"categories":null,"content":"","date":1599782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599782400,"objectID":"35283ed682647103d998c77f611d922e","permalink":"https://otaheri.github.io/example/2020-ballou-van-rooij-relationship/","publishdate":"2020-08-04T00:00:00Z","relpermalink":"/example/2020-ballou-van-rooij-relationship/","section":"example","summary":"Gaming disorder (also known as dysregulated gaming) has received significant research and policy attention based on concerns that certain patterns of play are associated with decreased mental well-being and/or functional impairment. In this study, we use specification curve analysis to examine analytical flexibility and the strength of the relationship between dysregulated gaming and well-being in the form of general mental health, depressive mood, and life satisfaction. Dutch and Flemish gamers (n = 424) completed ﬁve unique dysregulated gaming measures (covering nine scale variants) and three well-being measures. We find a consistent negative relationship; across 972 justifiable regression models, the median standardized regression coefficient was –0.40 (min: –0.54, max: –0.19). Data show that the majority of dysregulated gaming operationalizations converge upon highly similar estimates of well-being (i.e. have similar concurrent validity). However, variance is introduced by the choice of well-being measure; results indicate that dysregulated gaming is more strongly associated with depressive mood than with life satisfaction. Weekly gametime accounted for little to no unique variance in well-being in the sample. We argue that research on this topic should compare a broad range of functional and well-being outcomes, and work to identify a maximally parsimonious of dysregulated gaming criteria. Given somewhat minute differences between dysregulated gaming scales when used in survey-based studies and largely equivalent relationships with mental health indicators, harmonization of measurement should be a priority.","tags":null,"title":"The relationship between mental well-being and dysregulated gaming: A specification curve analysis of five gaming disorder scales","type":"example"},{"authors":["**Omid Taheri**","Nima Ghorbani","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While “grasping” is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of “whole-body grasps”. Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes.\nTL;DR We capture a very accurate dataset, named GRAB, of people interacting with 3D objects. We then use it to train a network, GrabNet, that generates hand grasp for novel objects.\nVideo Bloopers (Fun :D) Data and Code Please register and accept the License agreement on this website in order to get access to the GRAB dataset. The license and downloads section include explicit restrictions per subject, to which you agree to comply with.\nWhen creating an account, please opt-in for email communication, so that we can reach out to you per email to announce potential significant updates.\nGRAB dataset (works only after sign-in) GrabNet data(works only after sign-in) GrabNet model files/weights (works only after sign-in) Code for GRAB (GitHub) Code for GrabNet(GitHub)\nCitation @inproceedings{GRAB:2020, title = {{GRAB}: A Dataset of Whole-Body Human Grasping of Objects}, author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {European Conference on Computer Vision (ECCV)}, year = {2020}, url = {https://grab.is.tue.mpg.de} } ","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"491cb04acdcdd16f0a00f82f5a706027","permalink":"https://otaheri.github.io/publication/grab-a-dataset-of-whole-body-human-grasping-of-objects/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/grab-a-dataset-of-whole-body-human-grasping-of-objects/","section":"blog","summary":"Abstract Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time.","tags":[],"title":"GRAB: A Dataset of Whole-Body Human Grasping of Objects","type":"blog"},{"authors":["**Omid Taheri**","Nima Ghorbani","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While “grasping” is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of “whole-body grasps”. Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes.\nTL;DR We capture a very accurate dataset, named GRAB, of people interacting with 3D objects. We then use it to train a network, GrabNet, that generates hand grasp for novel objects.\nVideo Bloopers (Fun :D) Data and Code Please register and accept the License agreement on this website in order to get access to the GRAB dataset. The license and downloads section include explicit restrictions per subject, to which you agree to comply with.\nWhen creating an account, please opt-in for email communication, so that we can reach out to you per email to announce potential significant updates.\nGRAB dataset (works only after sign-in) GrabNet data(works only after sign-in) GrabNet model files/weights (works only after sign-in) Code for GRAB (GitHub) Code for GrabNet(GitHub)\nCitation @inproceedings{GRAB:2020, title = {{GRAB}: A Dataset of Whole-Body Human Grasping of Objects}, author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {European Conference on Computer Vision (ECCV)}, year = {2020}, url = {https://grab.is.tue.mpg.de} } ","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"7fc5dbfdc89cafd2d0f85e3062e456b5","permalink":"https://otaheri.github.io/publication/2020_grab/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2020_grab/","section":"publication","summary":"Abstract Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time.","tags":[],"title":"GRAB: A Dataset of Whole-Body Human Grasping of Objects","type":"publication"},{"authors":["David Zendle","Rachel Meyer","**Nick Ballou**"],"categories":null,"content":"","date":1588809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588809600,"objectID":"5b78e5980d4842d046513dbcad4068c0","permalink":"https://otaheri.github.io/example/2019-zendle-et-al-changing-face/","publishdate":"2020-01-04T13:58:37.322112Z","relpermalink":"/example/2019-zendle-et-al-changing-face/","section":"example","summary":"It is now common practice for video game companies to not just sell copies of games themselves, but to also sell in-game bonuses or items for a small real-world fee. These purchases may be purely aesthetic (cosmetic microtransactions); confer in-game advantages (pay to win microtransactions), or contain randomised contents of uncertain value (loot boxes).The growth of microtransactions has attracted substantial interest from both gamers, academics, and policymakers. However, it is not clear either how prevalent these features are in desktop games, or when any growth in prevalence occurred.In order to address this, we analysed the play history of the 463 most-played Steam desktop games from 2010 to 2019. Results of exploratory joinpoint analyses suggested that cosmetic microtransactions and loot boxes experienced rapid growth during 2012-2014, leading to high levels of prevalence by April 2019: 71.28% of the sample played games with loot boxes at this point, and 85.89% played games with cosmetic microtransactions. By contrast, pay to win microtransactions did not appear to experience similar growth in desktop games during the period, rising gradually to a prevalence of 17.38% by November 2015, at which point growth decelerated significantly (p\u003c0.001) to the point where it was not significantly different from zero (p=0.32).","tags":null,"title":"The Changing Face of Desktop Video Game Monetisation: An Exploration of Trends in Loot Boxes, Pay to Win, and Cosmetic Microtransactions in the Most-Played Steam Games of 2010-2019","type":"example"},{"authors":["David Zendle","Rachel Meyer","Paul Cairns","Stuart Waters","**Nick Ballou**"],"categories":null,"content":"","date":1579478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579478400,"objectID":"d12588fb761ffbfee6c43bcb624f71a2","permalink":"https://otaheri.github.io/example/2019-zendle-et-al-prevalence/","publishdate":"2020-01-04T13:58:37.320856Z","relpermalink":"/example/2019-zendle-et-al-prevalence/","section":"example","summary":"**Background and Aims** Loot boxes are items in video games that may be bought for real-world money but provide ran- domized rewards. Formal similarities between loot boxes and gambling have led to concerns that they may provide a ‘gateway’ to gambling amongst children. However, the availability of loot boxes is unclear. This study aimed to determine what proportion of top-grossing video games contained loot boxes, and how many of those games were available to children. **Design, setting and cases** Survey of the 100 top-grossing games on both the Google Play store and the Apple App store, and the top 50 most-played games on Steam according to the data aggregator SteamSpy. Measurements The prevalence of loot boxes was measured for each platform outlined above, split by age rating. **Findings** A total of 58.0% of the top games on the Google Play store contained loot boxes, 59.0% of the top iPhone games contained loot boxes and 36.0% of the top games on the Steam store contained loot boxes; 93.1% of the Android games that featured loot boxes and 94.9% of the iPhone games that featured loot boxes were deemed suitable for children aged 12+. Age ratings were more conservative for desktop games. Only 38.8% of desktop games that featured loot boxes were available to children aged 12+. **Conclusions** Loot boxes appear to be prevalent in video games that are deemed suitable for children, especially on mobile platforms.","tags":null,"title":"The Prevalence of Loot Boxes in Mobile and Desktop Games","type":"example"},{"authors":["**Omid Taheri**","Hassan Salarieh","Aria Alasty"],"categories":null,"content":"Abstract Human motion capture is frequently used to study rehabilitation and clinical problems, as well as to provide realistic animation for the entertainment industry. IMU-based systems, as well as Markerbased motion tracking systems, are most popular methods to track movement due to their low cost of implementation and lightweight. This paper proposes a quaternion-based Extended Kalman filter approach to recover the human leg segments motions with a set of IMU sensors data fused with cameramarker system data. In this paper, an Extended Kalman Filter approach is developed to fuse the data of two IMUs and one RGB camera for human leg motion tracking. Based on the complementary properties of the inertial sensors and camera-marker system, in the introduced new measurement model, the orientation data of the upper leg and the lower leg is updated through three measurement equations. The positioning of the human body is made possible by the tracked position of the pelvis joint by the camera marker system. A mathematical model has been utilized to estimate joints’ depth in 2D images. The efficiency of the proposed algorithm is evaluated by an optical motion tracker system.\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"f3140d1b388828545b876f6922f7df9c","permalink":"https://otaheri.github.io/publication/2019_leg_imu/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/2019_leg_imu/","section":"publication","summary":"Abstract Human motion capture is frequently used to study rehabilitation and clinical problems, as well as to provide realistic animation for the entertainment industry. IMU-based systems, as well as Markerbased motion tracking systems, are most popular methods to track movement due to their low cost of implementation and lightweight.","tags":[],"title":"Human Leg Motion Tracking by Fusing IMUs and RGB Camera Data Using Extended Kalman Filter","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne\rTwo\rThree\rA fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://otaheri.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://otaheri.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"}]