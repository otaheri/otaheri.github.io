
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":" Hello!\nI am a Ph.D. researcher in Perceiving Systems at the MPI for Intelligent Systems, advised by Professor Michael J. Black and Dr. Dimitrios Tzionas.\nI am interested in generating Virtual Humans that move and interact with 3D scenes and objects like Real Humans.\nMy broad research view includes: precise MoCap using multimodal sensors (IMUs, Cameras, Pressure Sensors, etc), 3D reconstruction from images, Human-Scene Interaction, and 3D representation.\nFeel free to contact me about my research, collaborations, or anything else here.\n","date":1667260800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1667260800,"objectID":"67a4dd131d2d6c0bdcd2e463c1e52b49","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hello!\nI am a Ph.D. researcher in Perceiving Systems at the MPI for Intelligent Systems, advised by Professor Michael J. Black and Dr. Dimitrios Tzionas.\nI am interested in generating Virtual Humans that move and interact with 3D scenes and objects like Real Humans.","tags":null,"title":"Omid Taheri","type":"authors"},{"authors":["Omid Taheri","Yi Zhou","Dimitrios Tzionas","Yang Zhou","Duygu Ceylan","Soren Pirk","Michael J. Black"],"categories":null,"content":"Abstract Hands are dexterous and highly versatile manipulators, that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality. To address this challenge, we propose GRIP, a learning-based method that synthesizes the motion of both left and right hands before, during, and after interaction with objects. We leverage the dynamics between the body and the object to extract two types of novel temporal interaction cues. We then define a two-stage inference pipeline in which GRIP first outputs the consistent interaction motion by taking a new approach to modeling motion temporal consistency in the latent space. In the second stage, GRIP generates refined hand poses to avoid finger-to-object penetrations. Both quantitative experiments and perceptual studies demonstrate that GRIP outperforms existing methods and generalizes to unseen objects and motions. Furthermore, GRIP achieves real-time performance with 45 fps hand pose prediction. Our models and code will be available for research purposes.\n","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"cc4b2fd6941d367fe8a5867780e085d7","permalink":"https://otaheri.github.io/publication/2023_grip/","publishdate":"2022-08-01T00:00:00Z","relpermalink":"/publication/2023_grip/","section":"publication","summary":"Abstract Hands are dexterous and highly versatile manipulators, that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality.","tags":[],"title":"GRIP: Generating Interaction Poses Conditioned on Object and Body Motion","type":"publication"},{"authors":["Zicong Fan","Omid Taheri","Dimitrios Tzionas","Muhammed Kocabas","Manuel Kaufmann","Michael J. Black","Otmar Hilliges"],"categories":null,"content":"Abstract We use our hands to interact with and to manipulate objects. Articulated objects are especially interesting since they often require the full dexterity of human hands to manipulate them. To understand, model, and synthesize such interactions, automatic and robust methods that reconstruct hands and articulated objects in 3D from a color image are needed. Existing methods for estimating 3D hand and object pose from images focus on rigid objects. In part, because such methods rely on training data and no dataset of articulated object manipulation exists. Consequently, we introduce ARCTIC – the first dataset of free-form interactions of hands and articulated objects. ARCTIC has 1.2M images paired with accurate 3D meshes for both hands and for objects that move and deform over time. The dataset also provides hand-object contact information. To show the value of our dataset, we perform two novel tasks on ARCTIC: (1) 3D reconstruction of two hands and an articulated object in interaction; (2) an estimation of dense hand-object relative distances, which we call interaction field estimation. For the first task, we present ArcticNet, a baseline method for the task of jointly reconstructing two hands and an articulated object from an RGB image. For interaction field estimation, we predict the relative distances from each hand vertex to the object surface, and vice versa. We introduce InterField, the first method that estimates such distances from a single RGB image. We provide qualitative and quantitative experiments for both tasks, and provide detailed analysis on the data.\nVideo ","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"3562e35d6d487017b16f8c99a4589e45","permalink":"https://otaheri.github.io/publication/2023_arctic/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/2023_arctic/","section":"publication","summary":"Abstract We use our hands to interact with and to manipulate objects. Articulated objects are especially interesting since they often require the full dexterity of human hands to manipulate them. To understand, model, and synthesize such interactions, automatic and robust methods that reconstruct hands and articulated objects in 3D from a color image are needed.","tags":[],"title":"ARCTIC: Articulated Objects in Free-form Hand Interaction","type":"publication"},{"authors":["Shashank Tripathi","Lea Müller","Chun-Hao P. Huang","Omid Taheri","Michael Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body’s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a “stable” configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH and Human3.6M show that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research.\nVideo ","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"190bb8792458b37e579095898f334131","permalink":"https://otaheri.github.io/publication/2023_ipman/","publishdate":"2022-12-02T00:00:00Z","relpermalink":"/publication/2023_ipman/","section":"publication","summary":"Abstract The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor.","tags":[],"title":"IPMAN: 3D Human Pose Estimation via Intuitive Physics","type":"publication"},{"authors":["Yinghao Huang","Omid Taheri","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies, ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images, while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images. Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are available for research purposes.\nVideo Data Please register and accept the License agreement on InterCap website in order to get access to the dataset.\nCitation @inproceedings{huang2022intercap, title = {{InterCap}: {J}oint Markerless {3D} Tracking of Humans and Objects in Interaction}, author = {Huang, Yinghao and Taheri, Omid and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {{German Conference on Pattern Recognition (GCPR)}}, volume = {13485}, pages = {281--299}, year = {2022}, organization = {Springer}, series = {Lecture Notes in Computer Science} } ","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"901fcc62162f6f671f78fb1360df5bdf","permalink":"https://otaheri.github.io/publication/2022_intercap/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022_intercap/","section":"publication","summary":"Abstract Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts.","tags":[],"title":"InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction","type":"publication"},{"authors":["Omid Taheri","Vasileios Choutas","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s generated motions approach the realism of GRAB’s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research.\nVideo Data and Code Please register and accept the License agreement on GRAB website in order to get access to the GRAB dataset.\nCitation @inproceedings{taheri2022goal, title = {{GOAL}: {G}enerating {4D} Whole-Body Motion for Hand-Object Grasping}, author = {Taheri, Omid and Choutas, Vasileios and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {Conference on Computer Vision and Pattern Recognition ({CVPR})}, year = {2022}, url = {https://goal.is.tue.mpg.de} } @inproceedings{GRAB:2020, title = {{GRAB}: {A} Dataset of Whole-Body Human Grasping of Objects}, author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {European Conference on Computer Vision ({ECCV})}, year = {2020}, url = {https://grab.is.tue.mpg.de} } ","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"f0c3bdbf060cff0e3631900404375d50","permalink":"https://otaheri.github.io/publication/2022_goal/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/2022_goal/","section":"publication","summary":"Abstract Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects.","tags":[],"title":"GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping","type":"publication"},{"authors":["Omid Taheri","Nima Ghorbani","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While “grasping” is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of “whole-body grasps”. Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes.\nTL;DR We capture a very accurate dataset, named GRAB, of people interacting with 3D objects. We then use it to train a network, GrabNet, that generates hand grasp for novel objects.\nVideo Bloopers (Fun :D) Data and Code Please register and accept the License agreement on this website in order to get access to the GRAB dataset. The license and downloads section include explicit restrictions per subject, to which you agree to comply with.\nWhen creating an account, please opt-in for email communication, so that we can reach out to you per email to announce potential significant updates.\nGRAB dataset (works only after sign-in) GrabNet data(works only after sign-in) GrabNet model files/weights (works only after sign-in) Code for GRAB (GitHub) Code for GrabNet(GitHub)\nCitation @inproceedings{GRAB:2020, title = {{GRAB}: A Dataset of Whole-Body Human Grasping of Objects}, author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {European Conference on Computer Vision (ECCV)}, year = {2020}, url = {https://grab.is.tue.mpg.de} } ","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"7fc5dbfdc89cafd2d0f85e3062e456b5","permalink":"https://otaheri.github.io/publication/2020_grab/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2020_grab/","section":"publication","summary":"Abstract Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time.","tags":[],"title":"GRAB: A Dataset of Whole-Body Human Grasping of Objects","type":"publication"},{"authors":["Omid Taheri","Hassan Salarieh","Aria Alasty"],"categories":null,"content":"Abstract Human motion capture is frequently used to study rehabilitation and clinical problems, as well as to provide realistic animation for the entertainment industry. IMU-based systems, as well as Markerbased motion tracking systems, are most popular methods to track movement due to their low cost of implementation and lightweight. This paper proposes a quaternion-based Extended Kalman filter approach to recover the human leg segments motions with a set of IMU sensors data fused with cameramarker system data. In this paper, an Extended Kalman Filter approach is developed to fuse the data of two IMUs and one RGB camera for human leg motion tracking. Based on the complementary properties of the inertial sensors and camera-marker system, in the introduced new measurement model, the orientation data of the upper leg and the lower leg is updated through three measurement equations. The positioning of the human body is made possible by the tracked position of the pelvis joint by the camera marker system. A mathematical model has been utilized to estimate joints’ depth in 2D images. The efficiency of the proposed algorithm is evaluated by an optical motion tracker system.\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"f3140d1b388828545b876f6922f7df9c","permalink":"https://otaheri.github.io/publication/2019_leg_imu/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/2019_leg_imu/","section":"publication","summary":"Abstract Human motion capture is frequently used to study rehabilitation and clinical problems, as well as to provide realistic animation for the entertainment industry. IMU-based systems, as well as Markerbased motion tracking systems, are most popular methods to track movement due to their low cost of implementation and lightweight.","tags":[],"title":"Human Leg Motion Tracking by Fusing IMUs and RGB Camera Data Using Extended Kalman Filter","type":"publication"}]