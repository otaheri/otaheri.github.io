
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"About Me Hello!\nI am a PostDoc researcher in Perceiving Systems at the MPI for Intelligent Systems, where I completed my Ph.D. under the guidance of Professor Michael J. Black and Dr. Dimitrios Tzionas. My research focuses on generating Virtual Humans that move and interact with 3D scenes and objects like Real Humans.\nMy research interests are broad and include precise motion capture (MoCap) using multimodal sensors (IMUs, Cameras, Pressure Sensors, etc.), 3D reconstruction from images, Human-Scene Interaction, and 3D representation. Recently, I’ve expanded my interests to include cutting-edge topics in AI such as diffusion models and Large Language Models (LLMs) for object interaction.\nFeel free to contact me about my research, collaborations, or anything else here.\n","date":1721044800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1721044800,"objectID":"67a4dd131d2d6c0bdcd2e463c1e52b49","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"About Me Hello!\nI am a PostDoc researcher in Perceiving Systems at the MPI for Intelligent Systems, where I completed my Ph.D. under the guidance of Professor Michael J. Black and Dr.","tags":null,"title":"Omid Taheri","type":"authors"},{"authors":null,"categories":null,"content":" ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c16c66f800c220b4b7396963b4b13ebf","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":" ","tags":null,"title":"Markos Diomataris","type":"authors"},{"authors":["Sai Kumar Dwivedi","Dimitrije Antić","Shashank Tripathi","Omid Taheri","Cordelia Schmid","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. We propose a Render-Localize-Lift module that:\nRender: Embeds 3D body and object surfaces into 2D space via multi-view rendering. Localize: Trains a multi-view localization model (MV-Loc) to infer precise 2D contact points. Lift: Projects the localized 2D contacts back to the 3D mesh. Additionally, we define a new task—Semantic Human Contact—which conditions contact estimation on object semantics, going beyond binary labels to infer object-specific interaction regions. InteractVLM significantly outperforms prior art on contact estimation benchmarks and facilitates joint 3D reconstruction from a single image.\nTeaser InteractVLM estimates 3D contact points on both human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. We introduce **Semantic Human Contact**, inferring object-specific contacts on the body, and leverage large Vision-Language Models for better generalization to diverse real-world interactions. Joint Human–Object Reconstruction Accurate joint 3D reconstruction of human and object from a single image.\nMethod Overview Comparison: InteractVLM vs. PHOSA Joint Reconstruction Comparison\nInput Image\nPHOSA\nInteractVLM\nSemantic Human Contact Estimation Contact Estimation Comparison\nInput Image\nDECO\nInteractVLM\nObject Affordance Prediction Affordance Comparison\nInput Image\nPIAD\nInteractVLM\nSummary Video Acknowledgments \u0026amp; Disclosure We thank Alpár Cseke for assistance with joint reconstruction evaluation; Tsvetelina Alexiadis and Taylor Obersat for MTurk studies; Yao Feng, Peter Kulits, Markos Diomataris for feedback; and Benjamin Pellkofer for IT support. SKD is funded by IMPRS-IS; UvA work by ERC Starting Grant STRIPES (101165317). DT received Google research funding. MJB’s involvement was solely supported by the Max Planck Society.\nContact For technical questions: sai.dwivedi@tue.mpg.de\nFor licensing: ps-licensing@tue.mpg.de\nBibTeX @inproceedings{dwivedi_interactvlm_2025, title = {{InteractVLM}: {3D} Interaction Reasoning from {2D} Foundational Models}, author = {Dwivedi, Sai Kumar and Antić, Dimitrije and Tripathi, Shashank and Taheri, Omid and Schmid, Cordelia and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2025}, } ","date":1743984000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743984000,"objectID":"506f7410c86aec0a8f4f49d85d518181","permalink":"https://otaheri.github.io/publication/2025_intervlm/","publishdate":"2025-04-07T00:00:00Z","relpermalink":"/publication/2025_intervlm/","section":"publication","summary":"Abstract We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes.","tags":[],"title":"InteractVLM: 3D Interaction Reasoning from 2D Foundational Models","type":"publication"},{"authors":["Shashank Tripathi","Omid Taheri","Christoph Lassner","Michael Black","Daniel Holden","Carsten Stoll"],"categories":null,"content":"Abstract Generating realistic human motion is crucial for many computer vision and graphics applications. The rich diversity of human body shapes and sizes significantly influences how people move. However, existing motion models typically overlook these differences, using a normalized, average body instead. This results in a homogenization of motion across human bodies, with motions not aligning with their physical attributes, thus limiting diversity. To address this, we propose a novel approach to learn a generative motion model conditioned on body shape. We demonstrate that it is possible to learn such a model from unpaired training data using cycle consistency, intuitive physics, and stability constraints that model the correlation between identity and movement.\nThe resulting model, HUMOS, generates natural, physically plausible, and dynamically stable human motions conditioned on body shape. More details are available on our project page.\n*Work done during an internship at Epic Games\n","date":1721044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721044800,"objectID":"80493781bb7c37a312247c872e603621","permalink":"https://otaheri.github.io/publication/2024_humos/","publishdate":"2024-07-15T12:00:00Z","relpermalink":"/publication/2024_humos/","section":"publication","summary":"Abstract Generating realistic human motion is crucial for many computer vision and graphics applications. The rich diversity of human body shapes and sizes significantly influences how people move. However, existing motion models typically overlook these differences, using a normalized, average body instead.","tags":[],"title":"HUMOS: Human Motion Model Conditioned on Body Shape","type":"publication"},{"authors":null,"categories":null,"content":" We are currently seeking passionate and dedicated interns to join our esteemed research group. This is a unique opportunity to work on pioneering research projects and gain invaluable experience in cutting-edge technologies.\nAreas of Research:\nHuman Motion Synthesis: Object Interaction Scene Interaction Reconstruction from Videos/Images: Humand and Objects Using Video Datasets for 3D Synthesis Video/3D Generation: 3D Representations Collecting Rich Datasets LLMs for Above Topics We are looking for candidates who have experience and a strong interest in these areas, preferably with papers in top-tier conferences such as CVPR, ICCV, ECCV, ICML, NeurIPS, ICLR, and others.\nThis is an excellent opportunity to collaborate with leading experts and contribute to groundbreaking research. If you are interested, please contact me.\n","date":1721044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721044800,"objectID":"e0e7a546a7845bb324a297df405865f1","permalink":"https://otaheri.github.io/positions/ps_intern_videogen/","publishdate":"2024-07-15T12:00:00Z","relpermalink":"/positions/ps_intern_videogen/","section":"positions","summary":"We are currently seeking passionate and dedicated interns to join our esteemed research group. This is a unique opportunity to work on pioneering research projects and gain invaluable experience in cutting-edge technologies.","tags":null,"title":"Internship Opportunity in Our Research Group","type":"positions"},{"authors":null,"categories":null,"content":"I have successfully defended my PhD thesis on July 4th, 2024. It was an incredible journey, and I am grateful for the support and guidance from my advisors, committee members, and colleagues.\nView Slides (Coming Soon) Read Thesis (Coming Soon) I am excited to embark on the next chapter of my career and continue contributing to the field. Thank you to everyone who has supported me along this journey.\nHere are some memorable moments from my PhD defense day:\n","date":1720094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720094400,"objectID":"9605ab68a366ddaddad97a528e879ceb","permalink":"https://otaheri.github.io/news/phd_finished/phd-finished/","publishdate":"2024-07-04T12:00:00Z","relpermalink":"/news/phd_finished/phd-finished/","section":"news","summary":"I have successfully defended my PhD thesis on July 4th, 2024. It was an incredible journey, and I am grateful for the support and guidance from my advisors, committee members, and colleagues.","tags":null,"title":"PhD Defense Completed - check the photo gallery!","type":"news"},{"authors":["Markos Diomataris","Nikos Athanasiou","Omid Taheri","Xi Wang","Otmar Hilliges","Michael J. Black"],"categories":null,"content":"Abstract Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness. A primary obstacle is the scarcity of training data that combines locomotion with goal reaching.\nTo address this, we introduce WANDR, a data-driven model that takes an avatar’s initial pose and a goal’s 3D position and generates natural human motions that place the end effector (wrist) on the goal location. To solve this, we introduce novel intention features that drive rich goal-oriented movement.\nIntention guides the agent to the goal, and interactively adapts the generation to novel situations without needing to define sub-goals or the entire motion path. Crucially, intention allows training on datasets that have goal-oriented motions as well as those that do not. WANDR is a conditional Variational Auto-Encoder (c-VAE), which we train using the AMASS and CIRCLE datasets. We evaluate our method extensively and demonstrate its ability to generate natural and long-term motions that reach 3D goals and generalize to unseen goal locations.\nWhat is WANDR? WANDR is a conditional Variational AutoEncoder (c-VAE) that generates realistic motion of human avatars that navigate towards an arbitrary goal location and reach for it.\nInput: The initial pose of the avatar, the goal location, and the desired motion duration.\nOutput: A sequence of poses that guide the avatar from the initial pose to the goal location and place the wrist on it.\n*Starting from the same state, WANDR generates diverse motions to reach different goal locations all around the human.* How is WANDR unique? WANDR is the first human motion generation model driven by an active feedback loop learned purely from data, without any extra steps of reinforcement learning (RL).\nActive closed loop guidance through intention features: WANDR generates motion autoregressively (frame-by-frame). At each step, it predicts a state-delta that will progress the human to the next state. The prediction of the state-delta is conditioned on time- and goal-dependent features that we call “intention” (visualized as arrows in videos below). These features are computed at every frame and act as a feedback loop that guides the motion generation to reach the goal. For more details on the intention, please refer to section 3.2 of the paper.\nPurely data-driven training: Existing datasets that capture motion of humans reaching for goals, like CIRCLE, are scarce and have very small scale to enable generalization. This is why RL is a popular approach to learn similar tasks. However, RL comes with its own set of challenges such as sample complexity. Inspired by the paradigm of behavioral cloning we propose a purely data-driven approach where during training a future position of the avatar’s hand is considered as the goal. By hallucinating goals this way, we are able to combine both smaller datasets with goal annotations such as CIRCLE, as well as large scale like AMASS that have no goal labels but are essential to learning general navigational skills such as walking, turning etc.\nMethod Our method is based on a conditional Variational Auto-Encoder (c-VAE) that learns to model motion as a frame-by-frame generation process by auto-encoding the pose difference between two adjacent frames. The condition signal consists of the human’s current pose and dynamics along with the intention information. Intention is a function of both the current pose and the goal location and therefore actively guides the avatar during the motion generation in a closed loop manner. Through training, the c-VAE learns the distribution of potential subsequent poses conditioned on the current dynamic state of the human and its intention towards a specific goal.\nWe train WANDR using two datasets: AMASS, which captures a wide range of motions including locomotion, and CIRCLE, which captures reaching motions. During inference, intention features are calculated based on the goal and act as a feedback loop that guides the motion generation towards the goal.\nAdapting to dynamic goals without training for it Since WANDR generates motion autoregressively, the intention features are updated at every frame. This allows the model to adapt to goals that move and change over time. Observe in the videos below how the intention features actively guide the avatar to orient itself towards the goal (orange arrow), get close to it (red arrow) and reach for it (blue arrow).\nWANDR generates motion autoregressively. This allows it to adapt to goals that move and change over time even though it has never been trained on scenarios with dynamic goals.\nVideo Citation @inproceedings{diomataris2024wandr, title = {{WANDR}: Intention-guided Human Motion Generation}, author = {Diomataris, Markos and Athanasiou, Nikos and Taheri, Omid and Wang, Xi …","date":1719792000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719792000,"objectID":"c6c1ac2509933b4722f17957a99ef4fb","permalink":"https://otaheri.github.io/publication/2024_wandr/","publishdate":"2024-07-01T00:00:00Z","relpermalink":"/publication/2024_wandr/","section":"publication","summary":"Abstract Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness.","tags":[],"title":"WANDR: Intention-guided Human Motion Generation","type":"publication"},{"authors":["Omid Taheri","Yi Zhou","Dimitrios Tzionas","Yang Zhou","Duygu Ceylan","Soren Pirk","Michael J. Black"],"categories":null,"content":"Abstract Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality. Prior work on capturing and modeling humans interacting with objects in 3D focuses on the body and object motion, often ignoring hand pose. In contrast, we introduce GRIP, a learning-based method that takes, as input, the 3D motion of the body and the object, and synthesizes realistic motion for both hands before, during, and after object interaction. As a preliminary step before synthesizing the hand motion, we first use a network, ANet, to denoise the arm motion. Then, we leverage the spatio-temporal relationship between the body and the object to extract novel temporal interaction cues, and use them in a two-stage inference pipeline to generate the hand motion. In the first stage, we introduce a new approach to encourage motion temporal consistency in the latent space (LTC) and generate consistent interaction motions. In the second stage, GRIP generates refined hand poses to avoid hand-object penetrations. Given sequences of noisy body and object motion, GRIP “upgrades” them to include hand-object interaction. Quantitative experiments and perceptual studies demonstrate that GRIP outperforms baseline methods and generalizes to unseen objects and motions from different motion-capture datasets. Our models and code are available for research purposes at GRIP.\nVideo Data and Code Please register and accept the License Agreement on this website to access the GRIP models. When creating an account, please opt-in for email communication, so that we can reach out to you via email to announce potential significant updates.\nModel files/weights (works only after sign-in) Code (GitHub) Results Input Motion 1 Output Motion 1 Input Motion 2 Output Motion 2 Proximity Sensor Ambient Sensor Citation @inproceedings{taheri2024grip, title = {{GRIP}: Generating Interaction Poses Using Latent Consistency and Spatial Cues}, author = {Omid Taheri and Yi Zhou and Dimitrios Tzionas and Yang Zhou and Duygu Ceylan and Soren Pirk and Michael J. Black}, booktitle = {International Conference on 3D Vision ({3DV})}, year = {2024}, url = {https://grip.is.tue.mpg.de} } ","date":1706745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"cdcc4aadfec54f78768043bd9ddd0ce3","permalink":"https://otaheri.github.io/publication/2024_grip/","publishdate":"2024-03-02T00:00:00Z","relpermalink":"/publication/2024_grip/","section":"publication","summary":"Abstract Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality.","tags":[],"title":"GRIP: Generating Interaction Poses Using Spatial Cues and Latent Consistency","type":"publication"},{"authors":["Yinghao Huang","Omid Taheri","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Humans constantly interact with objects to accomplish tasks. To understand such interactions, computers need to reconstruct these in 3D from images of whole bodies manipulating objects, e.g., for grasping, moving, and using the latter. This involves key challenges, such as occlusion between the body and objects, motion blur, depth ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community has followed a divide-and-conquer approach, focusing either only on interacting hands, ignoring the body, or on interacting bodies, ignoring the hands. However, these are only parts of the problem. On the contrary, recent work focuses on the whole problem. The GRAB dataset addresses whole-body interaction with dexterous hands but captures motion via markers and lacks video, while the BEHAVE dataset captures video of body-object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body SMPL-X model and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the body and object can be used to improve the pose estimation of both. (ii) Consumer-level Azure Kinect cameras let us set up a simple and flexible multi-view RGB-D system for reducing occlusions, with spatially calibrated and temporally synchronized cameras. With our InterCap method, we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 daily objects of various sizes and affordances, including contact with the hands or feet. To this end, we introduce a new data-driven hand motion prior, as well as explore simple ways for automatic contact detection based on 2D and 3D cues. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images, paired with pseudo ground-truth 3D body and object meshes. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are available for research purposes.\nVideo Data Please register and accept the License agreement on InterCap website in order to get access to the dataset.\nCitation @article{huang2024intercap, title = {{InterCap}: Joint Markerless {3D} Tracking of Humans and Objects in Interaction from Multi-view {RGB-D} Images}, author = {Huang, Yinghao and Taheri, Omid and Black, Michael J. and Tzionas, Dimitrios}, journal = {{International Journal of Computer Vision (IJCV)}}, volume = {}, number = {}, pages = {}, doi = {10.1007/s11263-024-01984-1}, year = {2024} } ","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"36369e279210e2199deee464dd4a20c2","permalink":"https://otaheri.github.io/publication/2024_intercap/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/2024_intercap/","section":"publication","summary":"Abstract Humans constantly interact with objects to accomplish tasks. To understand such interactions, computers need to reconstruct these in 3D from images of whole bodies manipulating objects, e.g., for grasping, moving, and using the latter.","tags":[],"title":"InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction","type":"publication"},{"authors":["Zicong Fan","Omid Taheri","Dimitrios Tzionas","Muhammed Kocabas","Manuel Kaufmann","Michael J. Black","Otmar Hilliges"],"categories":null,"content":"Abstract Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronized motion of hands and articulated objects. To this end, we introduce ARCTIC – a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively, and evaluate them qualitatively and quantitatively on ARCTIC.\nDexterous Motion + Dynamic Contact Annotation with MANO Annotation with SMPLX Rendered Depth Here we only visualize human + object for simplicity.\nCitation @inproceedings{fan2023arctic, title = {{ARCTIC}: A Dataset for Dexterous Bimanual Hand-Object Manipulation}, author = {Fan, Zicong and Taheri, Omid and Tzionas, Dimitrios and Kocabas, Muhammed and Kaufmann, Manuel and Black, Michael J. and Hilliges, Otmar}, booktitle = {Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, year = {2023} } ","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"3562e35d6d487017b16f8c99a4589e45","permalink":"https://otaheri.github.io/publication/2023_arctic/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/2023_arctic/","section":"publication","summary":"Abstract Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines.","tags":[],"title":"ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation","type":"publication"},{"authors":["Shashank Tripathi","Lea Müller","Chun-Hao P. Huang","Omid Taheri","Michael Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor. This is because most methods ignore the fact that bodies are typically supported by the scene. To address this, some methods exploit physics engines to enforce physical plausibility. Such methods, however, are not differentiable, rely on unrealistic proxy bodies, and are difficult to integrate into existing optimization and learning frameworks. To account for this, we take a different approach that exploits novel intuitive-physics (IP) terms that can be inferred from a 3D SMPL body interacting with the scene. Specifically, we infer biomechanically relevant features such as the pressure heatmap of the body on the floor, the Center of Pressure (CoP) from the heatmap, and the SMPL body’s Center of Mass (CoM) projected on the floor. With these, we develop IPMAN, to estimate a 3D body from a color image in a “stable” configuration by encouraging plausible floor contact and overlapping CoP and CoM. Our IP terms are intuitive, easy to implement, fast to compute, and can be integrated into any SMPL-based optimization or regression method; we show examples of both. To evaluate our method, we present MoYo, a dataset with synchronized multi-view color images and 3D bodies with complex poses, body-floor contact, and ground-truth CoM and pressure. Evaluation on MoYo, RICH, and Human3.6M shows that our IP terms produce more plausible results than the state of the art; they improve accuracy for static poses, while not hurting dynamic ones. Code and data will be available for research.\nVideo Data and Code Please register and accept the License agreement on the MoYo website in order to get access to the dataset.\nCitation @inproceedings{tripathi2023ipman, title = {{3D} Human Pose Estimation via Intuitive Physics}, author = {Tripathi, Shashank and M{\\\u0026#34;u}ller, Lea and Huang, Chun-Hao P. and Taheri Omid and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {Conference on Computer Vision and Pattern Recognition ({CVPR})}, pages = {4713--4725}, year = {2023}, url = {https://ipman.is.tue.mpg.de} } ","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"190bb8792458b37e579095898f334131","permalink":"https://otaheri.github.io/publication/2023_ipman/","publishdate":"2022-12-02T00:00:00Z","relpermalink":"/publication/2023_ipman/","section":"publication","summary":"Abstract The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor.","tags":[],"title":"IPMAN: 3D Human Pose Estimation via Intuitive Physics","type":"publication"},{"authors":["Yinghao Huang","Omid Taheri","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Humans constantly interact with objects to accomplish tasks. To understand such interactions, computers need to reconstruct these in 3D from images of whole bodies manipulating objects, e.g., for grasping, moving, and using the latter. This involves key challenges, such as occlusion between the body and objects, motion blur, depth ambiguities, and the low image resolution of hands and graspable object parts. To make the problem tractable, the community has followed a divide-and-conquer approach, focusing either only on interacting hands, ignoring the body, or on interacting bodies, ignoring the hands. However, these are only parts of the problem. On the contrary, recent work focuses on the whole problem. The GRAB dataset addresses whole-body interaction with dexterous hands but captures motion via markers and lacks video, while the BEHAVE dataset captures video of body-object interaction but lacks hand detail. We address the limitations of prior work with InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the parametric whole-body SMPL-X model and known object meshes. To tackle the above challenges, InterCap uses two key observations: (i) Contact between the body and object can be used to improve the pose estimation of both. (ii) Consumer-level Azure Kinect cameras let us set up a simple and flexible multi-view RGB-D system for reducing occlusions, with spatially calibrated and temporally synchronized cameras. With our InterCap method, we capture the InterCap dataset, which contains 10 subjects (5 males and 5 females) interacting with 10 daily objects of various sizes and affordances, including contact with the hands or feet. To this end, we introduce a new data-driven hand motion prior, as well as explore simple ways for automatic contact detection based on 2D and 3D cues. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images, paired with pseudo ground-truth 3D body and object meshes. Our InterCap method and dataset fill an important gap in the literature and support many research directions. Our data and code are available for research purposes.\nVideo Data Please register and accept the License agreement on InterCap website in order to get access to the dataset.\nCitation @article{huang2024intercap, title = {{InterCap}: Joint Markerless {3D} Tracking of Humans and Objects in Interaction from Multi-view {RGB-D} Images}, author = {Huang, Yinghao and Taheri, Omid and Black, Michael J. and Tzionas, Dimitrios}, journal = {{International Journal of Computer Vision (IJCV)}}, volume = {}, number = {}, pages = {}, doi = {10.1007/s11263-024-01984-1}, year = {2024} } @inproceedings{huang2022intercap, title = {{InterCap}: {J}oint Markerless {3D} Tracking of Humans and Objects in Interaction}, author = {Huang, Yinghao and Taheri, Omid and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {{German Conference on Pattern Recognition (GCPR)}}, volume = {13485}, pages = {281--299}, year = {2022}, organization = {Springer}, series = {Lecture Notes in Computer Science} } ","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"901fcc62162f6f671f78fb1360df5bdf","permalink":"https://otaheri.github.io/publication/2022_intercap/","publishdate":"2022-05-01T00:00:00Z","relpermalink":"/publication/2022_intercap/","section":"publication","summary":"Abstract Humans constantly interact with objects to accomplish tasks. To understand such interactions, computers need to reconstruct these in 3D from images of whole bodies manipulating objects, e.g., for grasping, moving, and using the latter.","tags":[],"title":"InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction","type":"publication"},{"authors":["Omid Taheri","Vasileios Choutas","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and, together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved because the avatar must look at the object to interact with it. For the first time, we address the problem of generating full-body, hand, and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object, its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks. First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact. Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets. We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to unseen objects, outperforming baselines. A perceptual study shows that GOAL’s generated motions approach the realism of GRAB’s ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research purposes.\nWhat is GOAL? GOAL is a generative model that generates full-body motion of the human body that walks and grasps unseen 3D objects. GOAL consists of two main steps:\nGNet generates the final grasp of the motion. MNet generates the motion from the starting to the grasp frame. It is trained on the GRAB dataset. For more details please refer to the Paper or the project website. GOAL generates diverse motions to reach different goal locations around the human.\nGNet Below you can see some generated whole-body static grasps from GNet. The hand close-ups are from the same grasp, and for better visualization:\nApple Binoculars Toothpaste MNet Below you can see some generated whole-body motions that walk and grasp 3D objects using MNet:\nCamera Mug Apple For more details check out the YouTube video below.\nData and Code Please register and accept the License agreement on GRAB website to get access to the GRAB dataset.\nCitation @inproceedings{taheri2022goal, title = {{GOAL}: {G}enerating {4D} Whole-Body Motion for Hand-Object Grasping}, author = {Taheri, Omid and Choutas, Vasileios and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {Conference on Computer Vision and Pattern Recognition ({CVPR})}, year = {2022}, url = {https://goal.is.tue.mpg.de} } @inproceedings{GRAB:2020, title = {{GRAB}: {A} Dataset of Whole-Body Human Grasping of Objects}, author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {European Conference on Computer Vision ({ECCV})}, year = {2020}, url = {https://grab.is.tue.mpg.de} } ","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"f0c3bdbf060cff0e3631900404375d50","permalink":"https://otaheri.github.io/publication/2022_goal/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publication/2022_goal/","section":"publication","summary":"Abstract Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects.","tags":[],"title":"GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping","type":"publication"},{"authors":["Omid Taheri","Nima Ghorbani","Michael J. Black","Dimitrios Tzionas"],"categories":null,"content":"Abstract Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While “grasping” is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of “whole-body grasps”. Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes.\nTL;DR We capture a very accurate dataset, named GRAB, of people interacting with 3D objects. We then use it to train a network, GrabNet, that generates hand grasps for novel objects.\nGRAB Dataset Dataset Overview GRAB is a dataset of full-body motions interacting and grasping 3D objects. It contains accurate finger and facial motions as well as the contact between the objects and body. The dataset includes 5 male and 5 female participants performing 4 different motion intents with 51 everyday objects.\nExample Motions Eat - Banana Talk - Phone Drink - Mug See - Binoculars The GRAB dataset also contains binary contact maps between the body and objects. With our interacting meshes, one could integrate these contact maps over time to create “contact heatmaps”, or even compute fine-grained contact annotations, as shown below:\nContact Heatmaps Contact Annotation Dataset Videos Long Video Short Video GrabNet Overview GrabNet is a generative model for 3D hand grasps. Given a 3D object mesh, GrabNet can predict several hand grasps for it. GrabNet has two successive models, CoarseNet (cVAE) and RefineNet. It is trained on a subset (right hand and object only) of the GRAB dataset.\nGenerated Results from GrabNet Binoculars Mug Camera Toothpaste GrabNet Videos Long Video Short Video Data and Code Please register and accept the License agreement on this website in order to get access to the GRAB dataset. The license and downloads section include explicit restrictions per subject, to which you agree to comply with.\nWhen creating an account, please opt-in for email communication, so that we can reach out to you per email to announce potential significant updates.\nGRAB dataset (works only after sign-in) GrabNet data (works only after sign-in) GrabNet model files/weights (works only after sign-in) Code for GRAB (GitHub) Code for GrabNet (GitHub) Citation @inproceedings{GRAB:2020, title = {{GRAB}: A Dataset of Whole-Body Human Grasping of Objects}, author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios}, booktitle = {European Conference on Computer Vision (ECCV)}, year = {2020}, url = {https://grab.is.tue.mpg.de} } ","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"7fc5dbfdc89cafd2d0f85e3062e456b5","permalink":"https://otaheri.github.io/publication/2020_grab/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/2020_grab/","section":"publication","summary":"Abstract Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time.","tags":[],"title":"GRAB: A Dataset of Whole-Body Human Grasping of Objects","type":"publication"},{"authors":["Omid Taheri","Hassan Salarieh","Aria Alasty"],"categories":null,"content":"Abstract Human motion capture is frequently used to study rehabilitation and clinical problems, as well as to provide realistic animation for the entertainment industry. IMU-based systems, as well as Markerbased motion tracking systems, are most popular methods to track movement due to their low cost of implementation and lightweight. This paper proposes a quaternion-based Extended Kalman filter approach to recover the human leg segments motions with a set of IMU sensors data fused with cameramarker system data. In this paper, an Extended Kalman Filter approach is developed to fuse the data of two IMUs and one RGB camera for human leg motion tracking. Based on the complementary properties of the inertial sensors and camera-marker system, in the introduced new measurement model, the orientation data of the upper leg and the lower leg is updated through three measurement equations. The positioning of the human body is made possible by the tracked position of the pelvis joint by the camera marker system. A mathematical model has been utilized to estimate joints’ depth in 2D images. The efficiency of the proposed algorithm is evaluated by an optical motion tracker system.\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"f3140d1b388828545b876f6922f7df9c","permalink":"https://otaheri.github.io/publication/2019_leg_imu/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/publication/2019_leg_imu/","section":"publication","summary":"Abstract Human motion capture is frequently used to study rehabilitation and clinical problems, as well as to provide realistic animation for the entertainment industry. IMU-based systems, as well as Markerbased motion tracking systems, are most popular methods to track movement due to their low cost of implementation and lightweight.","tags":[],"title":"Human Leg Motion Tracking by Fusing IMUs and RGB Camera Data Using Extended Kalman Filter","type":"publication"}]