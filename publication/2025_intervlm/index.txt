---
title: "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models"
toc: true

date: '2025-04-07T00:00:00Z'
doi: ''
view: 2
publishDate: '2025-04-07T00:00:00Z'

publication_types: ['1']
publication: In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*
publication_short: In *CVPR 2025*

image:
  placement: 1
  caption: 'Teaser: Semantic Human Contact and Joint 3D Reconstruction'

tags: []
authors: ["Sai Kumar Dwivedi", "Dimitrije Antić", "Shashank Tripathi", "Omid Taheri", "Cordelia Schmid", "Michael J. Black", "Dimitrios Tzionas"]

uurl_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

links:
  - icon_pack: fas
    icon: file-pdf
    name: PDF
    url: 'https://arxiv.org/pdf/2504.05303.pdf'
  - icon_pack: fas
    icon: scroll
    name: arXiv
    url: 'https://arxiv.org/abs/2504.05303'
  - icon_pack: fas
    icon: video
    name: Video
    url: 'https://www.youtube.com/watch?v=brxygxM1nRk'
  - icon_pack: fab
    icon: github
    name: Code
    url: 'https://github.com/saidwivedi/InteractVLM'
  - icon_pack: fas
    icon: file
    name: CVPR 2025

---

# Abstract

We introduce **InteractVLM**, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. We propose a **Render-Localize-Lift** module that:

1. **Render:** Embeds 3D body and object surfaces into 2D space via multi-view rendering.  
2. **Localize:** Trains a multi-view localization model (MV-Loc) to infer precise 2D contact points.  
3. **Lift:** Projects the localized 2D contacts back to the 3D mesh.  

Additionally, we define a new task—**Semantic Human Contact**—which conditions contact estimation on object semantics, going beyond binary labels to infer object-specific interaction regions. InteractVLM significantly outperforms prior art on contact estimation benchmarks and facilitates joint 3D reconstruction from a single image.

# Teaser

<img src="videos/teaser.jpg" class="center" />

<div style="text-align:center; margin-top:10px;">
  <span style="color:#007acc; font-weight:bold;">InteractVLM</span> estimates 3D contact points on both human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D.
  We introduce **Semantic Human Contact**, inferring object-specific contacts on the body, and leverage large Vision-Language Models for better generalization to diverse real-world interactions.
</div>

# Joint Human–Object Reconstruction

**Accurate joint 3D reconstruction of human and object from a single image.**

<video autoplay muted loop playsinline height="300">
  <source src="videos/joint_reconstruction.mp4" type="video/mp4">
</video>

# Method Overview

![Method Overview](videos/method.png)

# Comparison: InteractVLM vs. PHOSA

**Joint Reconstruction Comparison**  
<div style="display:flex; gap:10px; justify-content:center; flex-wrap: wrap;">
  <div style="flex:1 1 200px; text-align:center;">
    <img src="videos/bench.png" style="width:100%; border-radius:10px;" />
    <p><strong>Input Image</strong></p>
  </div>
  <div style="flex:1 1 200px; text-align:center;">
    <img src="videos/bench_phosa.gif" style="width:100%; border-radius:10px;" />
    <p><strong>PHOSA</strong></p>
  </div>
  <div style="flex:1 1 200px; text-align:center;">
    <img src="videos/bench_ivlm.gif" style="width:100%; border-radius:10px;" />
    <p><strong>InteractVLM</strong></p>
  </div>
</div>

# Semantic Human Contact Estimation

**Contact Estimation Comparison**  
<div style="display:flex; gap:10px; justify-content:center; flex-wrap: wrap;">
  <div style="flex:1 1 200px; text-align:center;">
    <img src="videos/hcontact_bench.jpg" style="width:100%; border-radius:10px;" />
    <p><strong>Input Image</strong></p>
  </div>
  <div style="flex:1 1 200px; text-align:center;">
    <img src="videos/hcontact_bench_deco.gif" style="width:100%; border-radius:10px;" />
    <p><strong>DECO</strong></p>
  </div>
  <div style="flex:1 1 200px; text-align:center;">
    <img src="videos/hcontact_bench_ivlm.gif" style="width:100%; border-radius:10px;" />
    <p><strong>InteractVLM</strong></p>
  </div>
</div>

# Object Affordance Prediction

**Affordance Comparison**  
<div style="display:flex; gap:10px; justify-content:center; flex-wrap: wrap;">
  <div style="flex:1 1 200px; text-align:center;">
    <img src="videos/ocontact_img1.png" style="width:100%; border-radius:10px;" />
    <p><strong>Input Image</strong></p>
  </div>
  <div style="flex:1 1 200px; text-align:center;">
    <img src="videos/ocontact_img1_piad.png" style="width:100%; border-radius:10px;" />
    <p><strong>PIAD</strong></p>
  </div>
  <div style="flex:1 1 200px; text-align:center;">
    <img src="videos/ocontact_img1_ivlm.png" style="width:100%; border-radius:10px;" />
    <p><strong>InteractVLM</strong></p>
  </div>
</div>

# Summary Video

<div style="text-align:center;">
  <iframe width="560" height="315"
    src="https://www.youtube.com/embed/brxygxM1nRk"
    frameborder="0"
    allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen>
  </iframe>
</div>

# Acknowledgments & Disclosure

We thank Alpár Cseke for assistance with joint reconstruction evaluation; Tsvetelina Alexiadis and Taylor Obersat for MTurk studies; Yao Feng, Peter Kulits, Markos Diomataris for feedback; and Benjamin Pellkofer for IT support. SKD is funded by IMPRS-IS; UvA work by ERC Starting Grant STRIPES (101165317). DT received Google research funding. MJB’s involvement was solely supported by the Max Planck Society.

# Contact

For technical questions: <sai.dwivedi@tue.mpg.de>  
For licensing: <ps-licensing@tue.mpg.de>

# BibTeX

```bibtex
@inproceedings{dwivedi_interactvlm_2025,
  title     = {{InteractVLM}: {3D} Interaction Reasoning from {2D} Foundational Models},
  author    = {Dwivedi, Sai Kumar and Antić, Dimitrije and Tripathi, Shashank and Taheri, Omid and Schmid, Cordelia and Black, Michael J. and Tzionas, Dimitrios},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2025},
}
