<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications | Omid Taheri</title>
    <link>https://otaheri.github.io/publication/</link>
      <atom:link href="https://otaheri.github.io/publication/index.xml" rel="self" type="application/rss+xml" />
    <description>Publications</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://otaheri.github.io/media/logo_huf4879d399e56855f7c1fe09e7af1fbc4_257295_300x300_fit_lanczos_3.png</url>
      <title>Publications</title>
      <link>https://otaheri.github.io/publication/</link>
    </image>
    
    <item>
      <title>GRIP: Generating Interaction Poses Conditioned on Object and Body Motion</title>
      <link>https://otaheri.github.io/publication/2023_grip/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2023_grip/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Hands are dexterous and highly versatile manipulators,
that are central to how humans interact with objects and
their environment. Consequently, modeling realistic hand
object interactions, including the subtle motion of individual
fingers, is critical for applications in computer graphics,
computer vision, and mixed reality. To address this
challenge, we propose GRIP, a learning-based method that
synthesizes the motion of both left and right hands before,
during, and after interaction with objects. We leverage the
dynamics between the body and the object to extract two
types of novel temporal interaction cues. We then define a
two-stage inference pipeline in which GRIP first outputs the
consistent interaction motion by taking a new approach to
modeling motion temporal consistency in the latent space.
In the second stage, GRIP generates refined hand poses to
avoid finger-to-object penetrations. Both quantitative experiments
and perceptual studies demonstrate that GRIP
outperforms existing methods and generalizes to unseen objects
and motions. Furthermore, GRIP achieves real-time
performance with 45 fps hand pose prediction. Our models
and code will be available for research purposes.&lt;/p&gt;
&lt;!--
# Video

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/A7b8DYovDZY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;


# Data and Code

Please register and accept the License agreement on [GRAB](https://grab.is.tue.mpg.de) website in order to get access to the GRAB dataset. 



# Citation

```
@inproceedings{taheri2022goal,
    title = {{GOAL}: {G}enerating {4D} Whole-Body Motion for Hand-Object Grasping},
    author = {Taheri, Omid and Choutas, Vasileios and Black, Michael J. and Tzionas, Dimitrios},
    booktitle = {Conference on Computer Vision and Pattern Recognition ({CVPR})},
    year = {2022},
    url = {https://goal.is.tue.mpg.de}
}

@inproceedings{GRAB:2020,
    title = {{GRAB}: {A} Dataset of Whole-Body Human Grasping of Objects},
    author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios},
    booktitle = {European Conference on Computer Vision ({ECCV})},
    year = {2020},
    url = {https://grab.is.tue.mpg.de}
}
```

--&gt;
</description>
    </item>
    
    <item>
      <title>ARCTIC: Articulated Objects in Free-form Hand Interaction</title>
      <link>https://otaheri.github.io/publication/2023_arctic/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2023_arctic/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;We use our hands to interact with and to manipulate objects. Articulated objects are especially interesting since they
often require the full dexterity of human hands to manipulate them. To understand, model, and synthesize such interactions,
automatic and robust methods that reconstruct hands and articulated objects in 3D from a color image are needed. Existing
methods for estimating 3D hand and object pose from images focus on rigid objects. In part, because such methods rely on
training data and no dataset of articulated object manipulation exists. Consequently, we introduce ARCTIC â€“ the first dataset
of free-form interactions of hands and articulated objects. ARCTIC has 1.2M images paired with accurate 3D meshes for
both hands and for objects that move and deform over time. The dataset also provides hand-object contact information.
To show the value of our dataset, we perform two novel tasks on ARCTIC: (1) 3D reconstruction of two hands and an articulated
object in interaction; (2) an estimation of dense hand-object relative distances, which we call interaction field estimation.
For the first task, we present ArcticNet, a baseline method for the task of jointly reconstructing two hands and an articulated object
from an RGB image. For interaction field estimation, we predict the relative distances from each hand vertex to the object surface,
and vice versa. We introduce InterField, the first method that estimates such distances from a single RGB image. We provide
qualitative and quantitative experiments for both tasks, and provide detailed analysis on the data.&lt;/p&gt;
&lt;h1 id=&#34;video&#34;&gt;Video&lt;/h1&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;iframe src=&#34;https://download.is.tue.mpg.de/arctic/video.mp4&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;!--
# Data and Code

Please register and accept the License agreement on [GRAB](https://grab.is.tue.mpg.de) website in order to get access to the GRAB dataset. 



# Citation

```

```
--&gt;
</description>
    </item>
    
    <item>
      <title>InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</title>
      <link>https://otaheri.github.io/publication/2022_intercap/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2022_intercap/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Humans constantly interact with daily objects to accomplish tasks. To understand such interactions, computers need to
reconstruct these from cameras observing whole-body interaction with scenes. This is challenging due to occlusion between
the body and objects, motion blur, depth/scale ambiguities, and the low image resolution of hands and graspable object parts.
To make the problem tractable, the community focuses either on interacting hands, ignoring the body, or on interacting bodies,
ignoring hands. The GRAB dataset addresses dexterous whole-body interaction but uses marker-based MoCap and lacks images,
while BEHAVE captures video of body object interaction but lacks hand detail. We address the limitations of prior work with
InterCap, a novel method that reconstructs interacting whole-bodies and objects from multi-view RGB-D data, using the
parametric whole-body model SMPL-X and known object meshes. To tackle the above challenges, InterCap uses two key observations:
(i) Contact between the hand and object can be used to improve the pose estimation of both. (ii) Azure Kinect sensors
allow us to set up a simple multi-view RGB-D capture system that minimizes the effect of occlusion while providing
reasonable inter-camera synchronization. With this method we capture the InterCap dataset, which contains 10 subjects
(5 males and 5 females) interacting with 10 objects of various sizes and affordances, including contact with the hands
or feet. In total, InterCap has 223 RGB-D videos, resulting in 67,357 multi-view frames, each containing 6 RGB-D images.
Our method provides pseudo ground-truth body meshes and objects for each video frame. Our InterCap method and dataset
fill an important gap in the literature and support many research directions. Our data and code are available for research purposes.&lt;/p&gt;
&lt;h1 id=&#34;video&#34;&gt;Video&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/d5wHLDIqN6c&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;Please register and accept the License agreement on &lt;a href=&#34;https://intercap.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InterCap&lt;/a&gt; website in order to get access to the dataset.&lt;/p&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;@inproceedings{huang2022intercap,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    title        = {{InterCap}: {J}oint Markerless {3D} Tracking of Humans and Objects in Interaction},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    author       = {Huang, Yinghao and Taheri, Omid and Black, Michael J. and Tzionas, Dimitrios},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    booktitle    = {{German Conference on Pattern Recognition (GCPR)}},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    volume       = {13485},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    pages        = {281--299},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    year         = {2022}, 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    organization = {Springer},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    series       = {Lecture Notes in Computer Science}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping</title>
      <link>https://otaheri.github.io/publication/2022_goal/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2022_goal/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus
on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on
generating realistic static grasps of objects. To synthesize virtual characters that interact with the world, we need to
generate full-body motions and realistic hand grasps simultaneously. Both sub-problems are challenging on their own and,
together, the state-space of poses is significantly larger, the scales of hand and body motions differ, and the whole-body
posture and the hand grasp must agree, satisfy physical constraints, and be plausible. Additionally, the head is involved
because the avatar must look at the object to interact with it. For the first time, we address the problem of generating
full-body, hand and head motions of an avatar grasping an unknown object. As input, our method, called GOAL, takes a 3D object,
its position, and a starting 3D body pose and shape. GOAL outputs a sequence of whole-body poses using two novel networks.
First, GNet generates a goal whole-body grasp with a realistic body, head, arm, and hand pose, as well as hand-object contact.
Second, MNet generates the motion between the starting and goal pose. This is challenging, as it requires the avatar to walk
towards the object with foot-ground contact, orient the head towards it, reach out, and grasp it with a realistic hand pose
and hand-object contact. To achieve this the networks exploit a representation that combines SMPL-X body parameters and 3D vertex offsets.
We train and evaluate GOAL, both qualitatively and quantitatively, on the GRAB dataset. Results show that GOAL generalizes well
to unseen objects, outperforming baselines. A perceptual study shows that GOALâ€™s generated motions approach the realism of GRABâ€™s
ground truth. GOAL takes a step towards synthesizing realistic full-body object grasping. Our models and code are available for research.&lt;/p&gt;
&lt;h1 id=&#34;video&#34;&gt;Video&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/A7b8DYovDZY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h1 id=&#34;data-and-code&#34;&gt;Data and Code&lt;/h1&gt;
&lt;p&gt;Please register and accept the License agreement on &lt;a href=&#34;https://grab.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GRAB&lt;/a&gt; website in order to get access to the GRAB dataset.&lt;/p&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;@inproceedings{taheri2022goal,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    title = {{GOAL}: {G}enerating {4D} Whole-Body Motion for Hand-Object Grasping},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    author = {Taheri, Omid and Choutas, Vasileios and Black, Michael J. and Tzionas, Dimitrios},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    booktitle = {Conference on Computer Vision and Pattern Recognition ({CVPR})},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    year = {2022},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    url = {https://goal.is.tue.mpg.de}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;@inproceedings{GRAB:2020,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    title = {{GRAB}: {A} Dataset of Whole-Body Human Grasping of Objects},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    booktitle = {European Conference on Computer Vision ({ECCV})},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    year = {2020},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    url = {https://grab.is.tue.mpg.de}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>GRAB: A Dataset of Whole-Body Human Grasping of Objects</title>
      <link>https://otaheri.github.io/publication/2020_grab/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2020_grab/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes,
detailed contact information, hand pose and shape, and the 3D body motion over time. While &amp;ldquo;grasping&amp;rdquo; is commonly thought of as a
single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of &amp;ldquo;whole-body grasps&amp;rdquo;.
Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of
10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose,
including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute
contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how
humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task.
We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network,
to predict 3D hand grasps for unseen 3D object shapes.&lt;/p&gt;
&lt;h1 id=&#34;tldr&#34;&gt;TL;DR&lt;/h1&gt;
&lt;p&gt;We capture a very accurate dataset, named GRAB, of people interacting with 3D objects. We then use it to train a network, GrabNet, that generates hand grasp for novel objects.&lt;/p&gt;
&lt;h1 id=&#34;video&#34;&gt;Video&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/s5syYMxmNHA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;bloopers-fun-d&#34;&gt;Bloopers (Fun :D)&lt;/h2&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;iframe src=&#34;https://www.youtube.com/embed/nRxRn4ZzQ60&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h1 id=&#34;data-and-code&#34;&gt;Data and Code&lt;/h1&gt;
&lt;p&gt;Please register and accept the License agreement on &lt;a href=&#34;https://grab.is.tue.mpg.de&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this&lt;/a&gt; website in order to get access to the GRAB dataset. The license and downloads section include explicit restrictions per subject, to which you agree to comply with.&lt;/p&gt;
&lt;p&gt;When creating an account, please opt-in for email communication, so that we can reach out to you per email to announce potential significant updates.&lt;/p&gt;
&lt;p&gt;GRAB dataset (works only after sign-in)
GrabNet data(works only after sign-in)
GrabNet model files/weights (works only after sign-in)
Code for GRAB (GitHub)
Code for GrabNet(GitHub)&lt;/p&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;@inproceedings{GRAB:2020,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  title = {{GRAB}: A Dataset of Whole-Body Human Grasping of Objects},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  author = {Taheri, Omid and Ghorbani, Nima and Black, Michael J. and Tzionas, Dimitrios},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  booktitle = {European Conference on Computer Vision (ECCV)},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  year = {2020},
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  url = {https://grab.is.tue.mpg.de}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Human Leg Motion Tracking by Fusing IMUs and RGB Camera Data Using Extended Kalman Filter</title>
      <link>https://otaheri.github.io/publication/2019_leg_imu/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://otaheri.github.io/publication/2019_leg_imu/</guid>
      <description>&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Human motion capture is frequently used to study rehabilitation and clinical problems, as well
as to provide realistic animation for the entertainment industry. IMU-based systems, as well as Markerbased
motion tracking systems, are most popular methods to track movement due to their low cost of
implementation and lightweight. This paper proposes a quaternion-based Extended Kalman filter
approach to recover the human leg segments motions with a set of IMU sensors data fused with
cameramarker system data. In this paper, an Extended Kalman Filter approach is developed to fuse the data of
two IMUs and one RGB camera for human leg motion tracking. Based on the complementary properties
of the inertial sensors and camera-marker system, in the introduced new measurement model, the
orientation data of the upper leg and the lower leg is updated through three measurement equations. The
positioning of the human body is made possible by the tracked position of the pelvis joint by the camera
marker system. A mathematical model has been utilized to estimate jointsâ€™ depth in 2D images. The
efficiency of the proposed algorithm is evaluated by an optical motion tracker system.&lt;/p&gt;
&lt;!--
# Video

&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;iframe src=&#34;https://download.is.tue.mpg.de/arctic/video.mp4&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;


# Data and Code

Please register and accept the License agreement on [GRAB](https://grab.is.tue.mpg.de) website in order to get access to the GRAB dataset. 



# Citation

```

```
--&gt;
</description>
    </item>
    
  </channel>
</rss>
