---
title: "ARCTIC: Articulated Objects in Free-form Hand Interaction"
toc: true

# Author notes (optional)
# (author_notes:)
# (  - 'Equal contribution')
# (  - 'Equal contribution')

date: '2022-10-01T00:00:00Z'
doi: ''
view: 2
# Schedule page publish date (NOT publication's date).
publishDate: '2022-12-01T00:00:00Z'

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ['1']

# Publication name and optional abbreviated publication name.

publication: In *Conference on Computer Vision and Pattern Recognition 2023*
publication_short: In *CVPR2023*


image:
  placement: 1
  caption: 'Teaser'

tags: []
authors: ["Zicong Fan", "otaheri", "Dimitrios Tzionas", "Muhammed Kocabas", "Manuel Kaufmann", "Michael J. Black", "Otmar Hilliges"]


url_pdf: 'https://arxiv.org/abs/2204.13662'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: 'https://arctic.is.tue.mpg.de/'
url_slides: ''
url_source: ''
url_video: 'https://download.is.tue.mpg.de/arctic/video.mp4'


# publication: "*Addiction*"
links:
  - icon_pack: fas
    icon: file
    name: CVPR2023
    url: '/publication/2023_arctic/'
  - icon_pack: fas
    icon: scroll
    name: Conference
    url: '/publication/2023_arctic/'
  
---

# Abstract

We use our hands to interact with and to manipulate objects. Articulated objects are especially interesting since they
often require the full dexterity of human hands to manipulate them. To understand, model, and synthesize such interactions,
automatic and robust methods that reconstruct hands and articulated objects in 3D from a color image are needed. Existing
methods for estimating 3D hand and object pose from images focus on rigid objects. In part, because such methods rely on
training data and no dataset of articulated object manipulation exists. Consequently, we introduce ARCTIC â€“ the first dataset 
of free-form interactions of hands and articulated objects. ARCTIC has 1.2M images paired with accurate 3D meshes for 
both hands and for objects that move and deform over time. The dataset also provides hand-object contact information. 
To show the value of our dataset, we perform two novel tasks on ARCTIC: (1) 3D reconstruction of two hands and an articulated 
object in interaction; (2) an estimation of dense hand-object relative distances, which we call interaction field estimation.
For the first task, we present ArcticNet, a baseline method for the task of jointly reconstructing two hands and an articulated object
from an RGB image. For interaction field estimation, we predict the relative distances from each hand vertex to the object surface,
and vice versa. We introduce InterField, the first method that estimates such distances from a single RGB image. We provide
qualitative and quantitative experiments for both tasks, and provide detailed analysis on the data.


# Video

<div style="text-align:center;">
<iframe src="https://download.is.tue.mpg.de/arctic/video.mp4" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>

<!--
# Data and Code

Please register and accept the License agreement on [GRAB](https://grab.is.tue.mpg.de) website in order to get access to the GRAB dataset. 



# Citation

```

```
-->
