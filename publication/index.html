<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: June 12, 2025 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.9.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto+Mono&family=Source+Sans+Pro:wght@200;300;400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.7c66feb5d800643487a0d7896d3ca491.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Omid Taheri" />





  

<meta name="description" content="The personal website of Omid Taheri." />



<link rel="alternate" hreflang="en-us" href="https://otaheri.github.io/publication/" />
<link rel="canonical" href="https://otaheri.github.io/publication/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu9b7be29231a05e8610b49408e09cf277_125589_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu9b7be29231a05e8610b49408e09cf277_125589_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#bf874b" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@omidtaherii" />
  <meta property="twitter:creator" content="@omidtaherii" />
<meta property="twitter:image" content="https://otaheri.github.io/media/icon_hu9b7be29231a05e8610b49408e09cf277_125589_512x512_fill_lanczos_center_3.png" />



  

<meta property="og:type" content="website" />
<meta property="og:site_name" content="Omid Taheri" />
<meta property="og:url" content="https://otaheri.github.io/publication/" />
<meta property="og:title" content="Publications | Omid Taheri" />
<meta property="og:description" content="The personal website of Omid Taheri." /><meta property="og:image" content="https://otaheri.github.io/media/icon_hu9b7be29231a05e8610b49408e09cf277_125589_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta property="og:updated_time" content="2025-04-07T00:00:00&#43;00:00" />
  










  
  
  

  
  
    <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="Omid Taheri" />
  

  


  
  <title>Publications | Omid Taheri</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="3a079e7dad19be978a318345a7749d34" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.93531aebe67b54bc51e02adf38549042.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Omid Taheri</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Omid Taheri</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#news"><span>News</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#positions"><span>Positions</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/uploads/cv.pdf"><span>CV</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/uploads/thesis.pdf"><span>Thesis</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
            
            <li class="nav-item d-none d-lg-inline-flex">
              <a class="nav-link" href="https://twitter.com/omidtaherii" data-toggle="tooltip" data-placement="bottom" title="Follow me on Twitter" target="_blank" rel="noopener" aria-label="Follow me on Twitter">
                <i class="fab fa-twitter" aria-hidden="true"></i>
              </a>
            </li>
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    
















  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search form-control form-control-sm" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            <option value=".pubtype-1">
              1
            </option>
            
            <option value=".pubtype-2">
              2
            </option>
            
            <option value=".pubtype-3">
              3
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2025">
              2025
            </option>
            
            <option value=".year-2024">
              2024
            </option>
            
            <option value=".year-2022">
              2022
            </option>
            
            <option value=".year-2021">
              2021
            </option>
            
            <option value=".year-2020">
              2020
            </option>
            
            <option value=".year-2019">
              2019
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2025_intervlm/" >InteractVLM: 3D Interaction Reasoning from 2D Foundational Models</a>
    </div>

    
    <a href="/publication/2025_intervlm/"  class="summary-link">
      <div class="article-style">
        Abstract We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Sai Kumar Dwivedi</span>, <span >
      Dimitrije Antić</span>, <span >
      Shashank Tripathi</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Cordelia Schmid</span>, <span >
      Michael J. Black</span>, <span >
      Dimitrios Tzionas</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2025_intervlm/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2504.05303.pdf" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2504.05303" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=brxygxM1nRk" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/saidwivedi/InteractVLM" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/" >
    <i class="fas fa-file mr-1"></i>CVPR 2025</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2025_intervlm/" >
        <img src="/publication/2025_intervlm/featured_hu5474e924a52d849ae1b3e857e5434b11_76391_a5bc1d4cb428554bff605c2464d6070c.webp" height="45" width="150"
            alt="InteractVLM: 3D Interaction Reasoning from 2D Foundational Models" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2025_lastexam/" >Humanity&#39;s Last Exam: A Multi-Modal Benchmark at the Frontier of Human Knowledge</a>
    </div>

    
    <a href="/publication/2025_lastexam/"  class="summary-link">
      <div class="article-style">
        Abstract Benchmarks are essential for tracking rapid LLM progress—but today’s models exceed 90% on tasks like MMLU, saturating existing exams.
We introduce Humanity’s Last Exam (HLE), a multi-modal, closed-ended benchmark spanning 2,500 questions across 100+ subjects at the frontier of human knowledge.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span class="author-highlighted">
      Omid Taheri</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2025_lastexam/cite.bib">
  Cite
</a>



  
  
    
  
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://scale.com/leaderboard/humanitys_last_exam" target="_blank" rel="noopener">
  Dataset
</a>












  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2504.XXXXX" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://scale.com/leaderboard/humanitys_last_exam" target="_blank" rel="noopener">
    <i class="fas fa-table mr-1"></i>SEAL LLM Leaderboards</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2025_lastexam/" >
        <img src="/publication/2025_lastexam/featured_hu40a1f9ead398ba9dd4560757191b05c0_61341_92ebbc619e3e5c604658556bb5a12f79.webp" height="65" width="150"
            alt="Humanity&#39;s Last Exam: A Multi-Modal Benchmark at the Frontier of Human Knowledge" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2025_nil/" >NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models</a>
    </div>

    
    <a href="/publication/2025_nil/"  class="summary-link">
      <div class="article-style">
        Abstract Acquiring physically plausible motor skills across diverse and unconventional morphologies—from humanoids to ants—is crucial for robotics and simulation.
We introduce No-data Imitation Learning (NIL), which:
Generates a reference video with a pretrained video diffusion model from a single simulation frame + text prompt.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Mert Albaba</span>, <span >
      Chenhao Li</span>, <span >
      Markos Diomataris</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Andreas Krause</span>, <span >
      Michael Black</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2025_nil/cite.bib">
  Cite
</a>













<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/arXiv:2503.10626" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2503.10626v1.pdf" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2503.10626v1" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2025_nil/" >
        <img src="/publication/2025_nil/featured_hu9858b86fd25c0a8003fd03fd9648f8d0_1102782_e2ff1a7613aad14404d6d868902f991f.webp" height="89" width="150"
            alt="NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2025">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2025_haptic/" >HaPTIC: Predicting 4D Hand Trajectory from Monocular Videos</a>
    </div>

    
    <a href="/publication/2025_haptic/"  class="summary-link">
      <div class="article-style">
        Abstract We present HaPTIC, an approach that infers coherent 4D hand trajectories from monocular videos. Current video-based hand pose reconstruction methods primarily focus on improving frame-wise 3D pose using adjacent frames rather than studying consistent 4D hand trajectories in space.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Yufei Ye</span>, <span >
      Yao Feng</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Haiwen Feng</span>, <span >
      Shubham Tulsiani</span>, <span >
      Michael J. Black</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2025_haptic/cite.bib">
  Cite
</a>





<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://judyye.github.io" target="_blank" rel="noopener">
  Project
</a>









<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://doi.org/10.48550/arXiv.2501.08329" target="_blank" rel="noopener">
  DOI
</a>


  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2501.08329" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="http://github.com/judyye/haptic" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2025_haptic/" >
        <img src="/publication/2025_haptic/featured_hued269020f06394116b3387ec38e041a8_4698579_4bd14b083d93f3c3ddf696e03d9f8bb7.webp" height="72" width="150"
            alt="HaPTIC: Predicting 4D Hand Trajectory from Monocular Videos" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2025_choir/" >CHOIR: A Versatile and Differentiable Hand-Object Interaction Representation</a>
    </div>

    
    <a href="/publication/2025_choir/"  class="summary-link">
      <div class="article-style">
        Abstract Synthesizing accurate hand–object interactions (HOI) is critical for AR/VR and vision tasks.
Existing dense–correspondence methods improve contact fidelity but lack full differentiability or generality.
We propose CHOIR, a versatile, fully differentiable interaction field:
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Théo Morales</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Gerard Lacey</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2025_choir/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2409.16855.pdf" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>Paper</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2409.16855" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/DubiousCactus/CHOIR" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2025_choir/" >
        <img src="/publication/2025_choir/featured_hue33050dfb64351bd779ea4520bf883fd_500318_69ecbb584768cc091d941f89003db683.webp" height="87" width="150"
            alt="CHOIR: A Versatile and Differentiable Hand-Object Interaction Representation" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2025_cwgrasp/" >CWGrasp: 3D Whole-Body Grasp Synthesis with Directional Controllability</a>
    </div>

    
    <a href="/publication/2025_cwgrasp/"  class="summary-link">
      <div class="article-style">
        Abstract Synthesizing 3D whole bodies that realistically grasp objects is crucial for animation, mixed reality, and robotics.
Key challenges include natural coordination between hand, body, and environment, and the scarcity of training data.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Georgios Paschalidis</span>, <span >
      Romana Wilschut</span>, <span >
      Dimitrije Antić</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Dimitrios Tzionas</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2025_cwgrasp/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2408.16770" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=d9a4C2GHIv0" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/gpaschalidis/CWGrasp" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/" >
    <i class="fas fa-file mr-1"></i>3DV 2025</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2025_cwgrasp/" >
        <img src="/publication/2025_cwgrasp/featured_hu352248412c01bef9dd02bd1fd4874658_1417598_ad8d3524e6d942c0ba3303f0dbf8ef5b.webp" height="67" width="150"
            alt="CWGrasp: 3D Whole-Body Grasp Synthesis with Directional Controllability" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2024_humos/" >HUMOS: Human Motion Model Conditioned on Body Shape</a>
    </div>

    
    <a href="/publication/2024_humos/"  class="summary-link">
      <div class="article-style">
        Abstract Generating realistic human motion is crucial for many computer vision and graphics applications. The rich diversity of human body shapes and sizes significantly influences how people move. However, existing motion models typically overlook these differences, using a normalized, average body instead.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Shashank Tripathi</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Christoph Lassner</span>, <span >
      Michael Black</span>, <span >
      Daniel Holden</span>, <span >
      Carsten Stoll</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2024_humos/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://youtu.be" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/CarstenEpic/humos" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/" >
    <i class="fas fa-file mr-1"></i>ECCV2024</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2024_humos/" >
        <img src="/publication/2024_humos/featured_hu5e6c7a3e62889fc255c2a2c688c376b6_673306_462aa4ca5b62c557fa91a8b7accd1bd6.webp" height="64" width="150"
            alt="HUMOS: Human Motion Model Conditioned on Body Shape" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2024_wandr/" >WANDR: Intention-guided Human Motion Generation</a>
    </div>

    
    <a href="/publication/2024_wandr/"  class="summary-link">
      <div class="article-style">
        Abstract Synthesizing natural human motions that enable a 3D human avatar to walk and reach for arbitrary goals in 3D space remains an unsolved problem with many applications. Existing methods (data-driven or using reinforcement learning) are limited in terms of generalization and motion naturalness.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Markos Diomataris</span>, <span >
      Nikos Athanasiou</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Xi Wang</span>, <span >
      Otmar Hilliges</span>, <span >
      Michael J. Black</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2024_wandr/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2404.15383.pdf" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2404.15383" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://youtu.be/9szizM-XUCg?si=16vQbgoCHcJ-Pz5f" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/markos-diomataris/wandr" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://wandr.is.tue.mpg.de/" target="_blank" rel="noopener">
    <i class="fas fa-project-diagram mr-1"></i>Project</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/" >
    <i class="fas fa-file mr-1"></i>CVPR2024</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2024_wandr/" >
        <img src="/publication/2024_wandr/featured_huf1b1a3686496990b788527b7c5c8eba4_38181_ba8169d1a0939bf4103a77fdd303387f.webp" height="84" width="150"
            alt="WANDR: Intention-guided Human Motion Generation" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2024_grip/" >GRIP: Generating Interaction Poses Using Spatial Cues and Latent Consistency</a>
    </div>

    
    <a href="/publication/2024_grip/"  class="summary-link">
      <div class="article-style">
        Abstract Hands are dexterous and highly versatile manipulators that are central to how humans interact with objects and their environment. Consequently, modeling realistic hand-object interactions, including the subtle motion of individual fingers, is critical for applications in computer graphics, computer vision, and mixed reality.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Yi Zhou</span>, <span >
      Dimitrios Tzionas</span>, <span >
      Yang Zhou</span>, <span >
      Duygu Ceylan</span>, <span >
      Soren Pirk</span>, <span >
      Michael J. Black</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2024_grip/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2308.11617.pdf" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2308.11617" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://youtu.be/IpIIQrdahYs" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/otaheri/GRIP" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://grip.is.tue.mpg.de/media/upload/GRIP_Poster_3DV_Final.pdf" target="_blank" rel="noopener">
    <i class="fas fa-image mr-1"></i>Poster</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://grip.is.tue.mpg.de/" target="_blank" rel="noopener">
    <i class="fas fa-project-diagram mr-1"></i>Project</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/" >
    <i class="fas fa-file mr-1"></i>3DV2024</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2024_grip/" >
        <img src="/publication/2024_grip/featured_hu66bc694fa77d418420e7f8f610ec20c4_8769540_83e7484a7a16598083ff6032db1c5d70.webp" height="78" width="150"
            alt="GRIP: Generating Interaction Poses Using Spatial Cues and Latent Consistency" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2024">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2024_intercap/" >InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</a>
    </div>

    
    <a href="/publication/2024_intercap/"  class="summary-link">
      <div class="article-style">
        Abstract Humans constantly interact with objects to accomplish tasks. To understand such interactions, computers need to reconstruct these in 3D from images of whole bodies manipulating objects, e.g., for grasping, moving, and using the latter.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Yinghao Huang</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Michael J. Black</span>, <span >
      Dimitrios Tzionas</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2024_intercap/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://intercap.is.tue.mpg.de/media/upload/main.pdf" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/article/10.1007/s11263-024-01984-1" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>Paper</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://youtu.be/d5wHLDIqN6c" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/YinghaoHuang91/InterCap/tree/master" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://intercap.is.tue.mpg.de/download.php" target="_blank" rel="noopener">
    <i class="fas fa-database mr-1"></i>Data</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://intercap.is.tue.mpg.de/" target="_blank" rel="noopener">
    <i class="fas fa-project-diagram mr-1"></i>Project</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://link.springer.com/article/10.1007/s11263-024-01984-1" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>IJCV2024</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2024_intercap/" >
        <img src="/publication/2024_intercap/featured_hud7ca2c8b904fda66aeee9e392e238a79_1700310_363dabff6d2d146f09c730c53e6ca93c.webp" height="51" width="150"
            alt="InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2023_arctic/" >ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation</a>
    </div>

    
    <a href="/publication/2023_arctic/"  class="summary-link">
      <div class="article-style">
        Abstract Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Zicong Fan</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Dimitrios Tzionas</span>, <span >
      Muhammed Kocabas</span>, <span >
      Manuel Kaufmann</span>, <span >
      Michael J. Black</span>, <span >
      Otmar Hilliges</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2023_arctic/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="http://download.is.tue.mpg.de/arctic/arctic_april_24.pdf" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2204.13662" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=bvMm8gfFbZ8" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/zc-alexfan/arctic" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code/Competition</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arctic.is.tue.mpg.de/download.php" target="_blank" rel="noopener">
    <i class="fas fa-database mr-1"></i>Data</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arctic.is.tue.mpg.de/" target="_blank" rel="noopener">
    <i class="fas fa-project-diagram mr-1"></i>Project</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/" >
    <i class="fas fa-file mr-1"></i>CVPR2023</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2023_arctic/" >
        <img src="/publication/2023_arctic/featured_hu2abe15513aa96262a5ffefaa1619f0ff_217249_c1b3b560a6ed1322bd0ef208671781e6.webp" height="70" width="150"
            alt="ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2023_ipman/" >IPMAN: 3D Human Pose Estimation via Intuitive Physics</a>
    </div>

    
    <a href="/publication/2023_ipman/"  class="summary-link">
      <div class="article-style">
        Abstract The estimation of 3D human body shape and pose from images has advanced rapidly. While the results are often well aligned with image features in the camera view, the 3D pose is often physically implausible; bodies lean, float, or penetrate the floor.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Shashank Tripathi</span>, <span >
      Lea Müller</span>, <span >
      Chun-Hao P. Huang</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Michael Black</span>, <span >
      Dimitrios Tzionas</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2023_ipman/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2303.18246" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=Dufvp_O0ziU" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/sha2nkt/ipman-r" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://moyo.is.tue.mpg.de/" target="_blank" rel="noopener">
    <i class="fas fa-database mr-1"></i>Data (MoYo)</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://drive.google.com/file/d/1n8QeOI_WRqcVDUMrB-lG2NCJURhBjppG/view?usp=sharing" target="_blank" rel="noopener">
    <i class="fas fa-image mr-1"></i>Poster</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ipman.is.tue.mpg.de/" target="_blank" rel="noopener">
    <i class="fas fa-project-diagram mr-1"></i>Project</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/" >
    <i class="fas fa-file mr-1"></i>CVPR2023</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2023_ipman/" >
        <img src="/publication/2023_ipman/featured_huf290a64dcec61ecd97122e06c0289ae9_4160870_88229cf1da3f48d4c4617d65c8219135.webp" height="102" width="150"
            alt="IPMAN: 3D Human Pose Estimation via Intuitive Physics" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2022_intercap/" >InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction</a>
    </div>

    
    <a href="/publication/2022_intercap/"  class="summary-link">
      <div class="article-style">
        Abstract Humans constantly interact with objects to accomplish tasks. To understand such interactions, computers need to reconstruct these in 3D from images of whole bodies manipulating objects, e.g., for grasping, moving, and using the latter.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span >
      Yinghao Huang</span>, <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Michael J. Black</span>, <span >
      Dimitrios Tzionas</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2022_intercap/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://intercap.is.tue.mpg.de/media/upload/main.pdf" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2209.12354" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://youtu.be/d5wHLDIqN6c" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/YinghaoHuang91/InterCap/tree/master" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://intercap.is.tue.mpg.de/download.php" target="_blank" rel="noopener">
    <i class="fas fa-database mr-1"></i>Data</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://intercap.is.tue.mpg.de/media/upload/poster.pdf" target="_blank" rel="noopener">
    <i class="fas fa-image mr-1"></i>Poster</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://intercap.is.tue.mpg.de/" target="_blank" rel="noopener">
    <i class="fas fa-project-diagram mr-1"></i>Project</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/" >
    <i class="fas fa-file mr-1"></i>GCPR2022</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2022_intercap/" >
        <img src="/publication/2022_intercap/featured_hu68a2a018d3990d2e4634eec4898b14f6_146998_b43ace564ff1e9857261726c2e2dbab1.webp" height="56" width="150"
            alt="InterCap: Joint Markerless 3D Tracking of Humans and Objects in Interaction" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2022_goal/" >GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping</a>
    </div>

    
    <a href="/publication/2022_goal/"  class="summary-link">
      <div class="article-style">
        Abstract Generating digital humans that move realistically has many applications and is widely studied, but existing methods focus on the major limbs of the body, ignoring the hands and head. Hands have been separately studied but the focus has been on generating realistic static grasps of objects.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Vasileios Choutas</span>, <span >
      Michael J. Black</span>, <span >
      Dimitrios Tzionas</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2022_goal/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2112.11454.pdf" target="_blank" rel="noopener">
    <i class="fas fa-file-pdf mr-1"></i>PDF</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2112.11454" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://youtu.be/A7b8DYovDZY" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/otaheri/GOAL" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/otaheri/GOAL/blob/main/GOAL_Poster_CVPR_final.pdf" target="_blank" rel="noopener">
    <i class="fas fa-image mr-1"></i>Poster</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://goal.is.tue.mpg.de/" target="_blank" rel="noopener">
    <i class="fas fa-project-diagram mr-1"></i>Project</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/" >
    <i class="fas fa-file mr-1"></i>CVPR2022</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2022_goal/" >
        <img src="/publication/2022_goal/featured_hua789659adf90acfec49b8cbb169a8e54_3881880_697087e500c2ea7f190f5a7415585dc8.webp" height="70" width="150"
            alt="GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2020_grab/" >GRAB: A Dataset of Whole-Body Human Grasping of Objects</a>
    </div>

    
    <a href="/publication/2020_grab/"  class="summary-link">
      <div class="article-style">
        Abstract Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Nima Ghorbani</span>, <span >
      Michael J. Black</span>, <span >
      Dimitrios Tzionas</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2020_grab/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2008.11200" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://youtu.be/s5syYMxmNHA" target="_blank" rel="noopener">
    <i class="fas fa-video mr-1"></i>Video</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/otaheri/GRAB" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>GRAB</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/otaheri/GrabNet" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>GrabNet</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://grab.is.tue.mpg.de/download.php" target="_blank" rel="noopener">
    <i class="fas fa-database mr-1"></i>Data</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://grab.is.tue.mpg.de" target="_blank" rel="noopener">
    <i class="fas fa-project-diagram mr-1"></i>Project</a>

  
  
  
    
  
  
  
  
  
    
    
      
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="/" >
    <i class="fas fa-file mr-1"></i>ECCV2020</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2020_grab/" >
        <img src="/publication/2020_grab/featured_hu5a399f5f28d6cce565fe12ebc6a64e2b_1238609_f0eedc64b9f87289b0095ef6df764986.webp" height="105" width="150"
            alt="GRAB: A Dataset of Whole-Body Human Grasping of Objects" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2019">
          







  







  


<div class="media stream-item view-compact">
  <div class="media-body">

    <div class="section-subheading article-title mb-0 mt-0">
      <a href="/publication/2019_leg_imu/" >Human Leg Motion Tracking by Fusing IMUs and RGB Camera Data Using Extended Kalman Filter</a>
    </div>

    
    <a href="/publication/2019_leg_imu/"  class="summary-link">
      <div class="article-style">
        Abstract Human motion capture is frequently used to study rehabilitation and clinical problems, as well as to provide realistic animation for the entertainment industry. IMU-based systems, as well as Markerbased motion tracking systems, are most popular methods to track movement due to their low cost of implementation and lightweight.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  <span class="author-highlighted">
      Omid Taheri</span>, <span >
      Hassan Salarieh</span>, <span >
      Aria Alasty</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  





<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal"
        data-filename="/publication/2019_leg_imu/cite.bib">
  Cite
</a>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2011.00574" target="_blank" rel="noopener">
    <i class="fas fa-scroll mr-1"></i>arXiv</a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
      
      
      <a href="/publication/2019_leg_imu/" >
        <img src="/publication/2019_leg_imu/featured_hua884239c66bbb2e50144de91b32f31e1_118760_9b050d95610fe7b472a1cfdcefc0449e.webp" height="121" width="150"
            alt="Human Leg Motion Tracking by Fusing IMUs and RGB Camera Data Using Extended Kalman Filter" loading="lazy">
      </a>
    
  </div>
</div>

        </div>

        
      </div>

    </div>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  


<p class="powered-by">
  
  <a href="/privacy/">Privacy Policy</a>
  
  
   &middot; 
  <a href="/terms/">Terms</a>
  
</p>












  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2025 Omid Taheri. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.f64289d8217e08e3afcd597d60836062.js"></script>




  
    <script src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.82efd0fe1287c491a256693d281ecc98.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js" type="module"></script>


















</body>
</html>
